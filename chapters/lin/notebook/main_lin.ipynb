{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Solving linear systems  -->\n",
    "# Solving linear systems \n",
    "<!-- dom:AUTHOR: Aksel Hiorth, the National IOR Centre & Institute for Energy Resources, -->\n",
    "<!-- Author: -->  \n",
    "**Aksel Hiorth, the National IOR Centre & Institute for Energy Resources,**\n",
    "University of Stavanger\n",
    "\n",
    "Date: **Oct 27, 2020**\n",
    "\n",
    "<!-- Common Mako variables and functions -->\n",
    "\n",
    "\n",
    "\n",
    "Solving systems of equations are one of the most common tasks that we use computers for within modeling. A typical task is that we have a model that contains a set of unknown parameters which we want to determine. To determine these parameters we need to solve a set of equations. In many cases these equations are nonlinear, but often a nonlinear problem is solved\n",
    "*by linearize* the nonlinear equations, and thereby reducing it to a sequence of linear algebra problems. Thus the topic of solving linear systems of equations have been extensively studied, and sophisticated linear equation solving packages have been developed. Python uses functions from the [LAPACK](https://en.wikipedia.org/wiki/LAPACK) library. In this course we will only cover the theory behind numerical linear algebra superficially, and the main purpose is to shed some light on some of the challenges one might encounter solving linear systems. In particular it is important for you to understand when it is stated in the NumPy documentation that the standard linear solver: [`solve`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html) function uses *LU-decomposition* and *partial pivoting*. \n",
    "\n",
    "<!-- After covering some basics of numerical linear algebra, we will shift focus to nonlinear equations. Contrary to linear equations, you will most likely find that the functions available in various Python library will *not* cover your needs and in many cases fail to give you the correct solution. The reason for this is that the solution of a nonlinear equation is greatly dependent on the starting point, and a combination of various techniques must be used. -->\n",
    "\n",
    "# Solving linear equations\n",
    "There are a number of excellent books covering this topic, see e.g. [[press2007;@trefethen1997;@stoer2013;@strang2019]](#press2007;@trefethen1997;@stoer2013;@strang2019).\n",
    "In most of the examples covered in this course we will encounter problems where we have a set of *linearly independent* equations and one equation for each unknown. For these type of problems there are a number of methods that can be used, and they will find a solution in a finite number of steps. If a solution cannot be found it is usually because the equations are not linearly independent, and our formulation of the physical problem is wrong.\n",
    "\n",
    "Assume that we would like to solve the following set of equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:la\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "2x_0+x_1+x_2+3x_3=1,\\label{eq:lin:la} \\tag{1} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:lb\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_0+x_1+3x_2+x_3=-3,\\label{eq:lin:lb} \\tag{2} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:lc\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_0+4x_1+x_2+x_3=2,\\label{eq:lin:lc} \\tag{3} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ld\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_0+x_1+x_2+x_3=1.\\label{eq:lin:ld} \\tag{4} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equations can be written in matrix form as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:mat\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A\\cdot x}=\\mathbf{b},\n",
    "\\label{eq:lin:mat} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:matA\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\equiv\\begin{pmatrix}\n",
    "2&1&1&3\\\\ \n",
    "1&1&3&1\\\\ \n",
    "1&4&1&1\\\\ \n",
    "1&1&2&2\n",
    "\\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{b}\\equiv\n",
    "\\begin{pmatrix}\n",
    "1\\\\-3\\\\2\\\\1\n",
    "\\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{x}\\equiv\n",
    "\\begin{pmatrix}\n",
    "x_0\\\\x_1\\\\x_2\\\\x_3\n",
    "\\end{pmatrix}.\n",
    "\\label{eq:lin:matA} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily verify that $x_0=-4, x_1=1, x_2=-1, x_3= 3$ is the\n",
    "solution to the above equations by direct substitution. If we were to\n",
    "replace one of the above equations with a linear combination of any of\n",
    "the other equations, e.g. replace equation ([4](#eq:lin:ld)) with\n",
    "$3x_0+2x_1+4x_2+4x_3=-2$, there would be no unique solution (infinite\n",
    "number of solutions). This can be checked by calculating the determinant of the matrix $\\mathbf{A}$, if $\\det \\mathbf{A}=0 $,  \n",
    "What is the difficulty in solving these equations? Clearly if none of the equations are linearly dependent, and we have $N$ independent linear equations, it should be straight forward to solve them? Two major numerical problems are i) even if the equations are not exact linear combinations of each other, they could be very close, and as the numerical algorithm progresses they could at some stage become linearly dependent due to roundoff errors. ii) roundoff errors may accumulate if the number of equations are large [[press2007]](#press2007).\n",
    "\n",
    "## Gauss-Jordan elimination\n",
    "Let us continue the discussion by consider Gauss-Jordan elimination, which is a *direct* method. A direct method uses a final set of operations to obtain a solution. According to [[press2007]](#press2007) Gauss-Jordan elimination is the method of choice if we want to find the inverse of $\\mathbf{A}$. However, it is slow when it comes to calculate the solution of equation\n",
    "([5](#eq:lin:mat)). Even if speed and memory use is not an issue, it is also not advised to first find the inverse, $\\mathbf{A}^{-1}$, of $\\mathbf{A}$, then multiply it with $\\mathbf{b}$ to obtain the solution, due to roundoff errors (Roundoff errors occur whenever we subtract to numbers that are very close to each other). To simplify our notation, we write equation ([6](#eq:lin:matA)) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\begin{array}{cccc|c}\n",
    "2&1&1&3&1\\\\ \n",
    "1&1&3&1&-3\\\\ \n",
    "1&4&1&1&2\\\\ \n",
    "1&1&2&2&1\n",
    "\\end{array}\n",
    "\\right).\n",
    "\\label{_auto1} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers to the left of the vertical dash is the matrix $\\mathbf{A}$, and to the right is the vector $\\mathbf{b}$. The Gauss-Jordan elimination procedure proceeds by doing the same operation on the right and left side of the dash, and the goal is to get only zeros on the lower triangular part of the matrix. This is achieved by multiplying rows with the same (nonzero) number, swapping rows, adding a multiple of a row to another:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:gj1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\begin{array}{cccc|c}\n",
    "2&1&1&3&1\\\\ \n",
    "1&1&3&1&-3\\\\ \n",
    "1&4&1&1&2\\\\ \n",
    "1&1&2&2&1\n",
    "\\end{array}\n",
    "\\right)\\to\n",
    "\\left(\n",
    "\\begin{array}{cccc|c}\n",
    "2&1&1&3&1\\\\ \n",
    "0&1/2&5/2&-1/2&-7/2\\\\ \n",
    "0&7/2&1/2&-1/2&3/2\\\\ \n",
    "0&1/2&3/2&1/2&1/2\n",
    "\\end{array}\n",
    "\\right)\\to\\label{eq:lin:gj1} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\left(\n",
    "\\begin{array}{cccc|c}\n",
    "2&1&1&3&1\\\\ \n",
    "0&1/2&5/2&-1/2&-7/2\\\\ \n",
    "0&0&-17&3&26\\\\ \n",
    "0&0&1&-1&4\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\to\n",
    "\\left(\n",
    "\\begin{array}{cccc|c}\n",
    "2&1&1&3&1\\\\ \n",
    "0&1/2&5/2&-1/2&-7/2\\\\ \n",
    "0&0&-17&3&26\\\\ \n",
    "0&0&0&14/17&42/17\n",
    "\\end{array}\n",
    "\\right){\\nonumber}\n",
    "\\label{_auto2} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations done are: ($1\\to2$) multiply first row with $-1/2$ and add to second, third and the fourth row, ($2\\to 3$) multiply second row with $-7$, and add to third row, multiply second row with $-1$ and add to fourth row, ($3\\to4$) multiply third row with $-1/17$ and add to fourth row. These operations can easily be coded into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[2, 1, 1, 3,],[1, 1, 3, 1],\n",
    "              [1, 4, 1, 1, ],[1, 1, 2, 2 ]],float)\n",
    "b = np.array([1,-3,2,1],float)\n",
    "N=4\n",
    "# Gauss-Jordan Elimination\n",
    "for i in range(1,N):\n",
    "    fact    = A[i:,i-1]/A[i-1,i-1]\n",
    "    A[i:,] -= np.outer(fact,A[i-1,])\n",
    "    b[i:]  -= b[i-1]*fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the final matrix has only zeros beyond the diagonal, such a matrix is called *upper triangular*. We still have not found the final solution, but from an upper triangular (or lower triangular) matrix it is trivial to determine the solution. The last row immediately gives us $14/17z=42/17$ or $z=3$, now we have the solution for z and the next row gives: $-17y+3z=26$ or $y=(26-3\\cdot3)/(-17)=-1$, and so on. In a more general form, we can write our solution of the matrix $\\mathbf{A}$ after making it upper triangular as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:back\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "a^\\prime_{0,0}&a^\\prime_{0,1}&a^\\prime_{0,2}&a^\\prime_{0,3}\\\\ \n",
    "0&a^\\prime_{1,1}&a^\\prime_{1,2}&a^\\prime_{1,3}\\\\ \n",
    "0&0&a^\\prime_{2,2}&a^\\prime_{2,3}\\\\ \n",
    "0&0&0&a^\\prime_{3,3}\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "x_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "b^\\prime_{0}\\\\ \n",
    "b^\\prime_{1}\\\\ \n",
    "b^\\prime_{2}\\\\ \n",
    "b^\\prime_{3}\n",
    "\\end{pmatrix}\n",
    "\\label{eq:lin:back} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backsubstitution can then be written formally as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:back2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_i=\\frac{1}{a^\\prime_{ii}}\\left[b_i^\\prime-\\sum_{j=i+1}^{N-1}a^\\prime_{ij}x_j\\right],\\quad i=N-1,N-2,\\ldots,0\n",
    "\\label{eq:lin:back2} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backsubstitution can now easily be implemented in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Backsubstitution\n",
    "sol = np.zeros(N,float)\n",
    "sol[N-1]=b[N-1]/A[N-1,N-1]\n",
    "for i in range(2,N+1):\n",
    "    sol[N-i]=(b[N-i]-np.dot(A[(N-i),],sol))/A[N-i,N-i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the Python implementation, we have used vector operations instead of for loops. This makes the code more efficient, but it could also be implemented with for loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Backsubstitution - for loop\n",
    "sol = np.zeros(N,float)\n",
    "for i in range(N-1,-1,-1):\n",
    "    sol[i]= b[i]\n",
    "    for j in range(i+1,N):\n",
    "        sol[i] -= A[i][j]*sol[j]\n",
    "    sol[i] /= A[i][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two things to notice with our implementation:\n",
    "* Matrix and vector notation makes the code more compact and efficient. In order to understand the implementation it is advised to put $i=1, 2, 3, 4$, and then execute the statements in the Gauss-Jordan elimination and compare with equation ([8](#eq:lin:gj1)).\n",
    "\n",
    "* The implementation of the Gauss-Jordan elimination is not robust, in particular one could easily imagine cases where one of the leading coefficients turned out as zero, and the routine would fail when we divide by `A[i-1,i-1]`. By simply changing equation ([2](#eq:lin:lb)) to $2x_0+x_1+3x_2+x_3=-3$, when doing the first Gauss-Jordan elimination, both $x_0$ and $x_1$ would be canceled. In the next iteration we try to divide next equation by the leading coefficient of $x_1$, which is zero, and the whole procedure fails.\n",
    "\n",
    "## Pivoting\n",
    "The solution to the last problem is solved by what is called *pivoting*. The element that we divide on is called the *pivot element*. It actually turns out that even if we do Gauss-Jordan elimination *without* encountering a zero pivot element, the Gauss-Jordan procedure is numerically unstable in the presence of roundoff errors [[press2007]](#press2007). There are two versions of pivoting, *full pivoting* and *partial pivoting*. In partial pivoting we only interchange rows, while in full pivoting we also interchange rows and columns. Partial pivoting is much easier to implement, and the algorithm is as follows:\n",
    "1. Find the row in $\\mathbf{A}$ with largest absolute value in front of $x_0$ and change with the first equation, switch corresponding elements in $\\mathbf{b}$\n",
    "\n",
    "2. Do one Gauss-Jordan elimination, find the row in $\\mathbf{A}$ with the largest absolute value in front of $x_1$ and switch with the second (same for $\\mathbf{b}$), and so on.\n",
    "\n",
    "For a linear equation we can multiply with a number on each side and the equation would be unchanged, so if we where to multiply one of the equations with a large value, we are almost sure that this equation would be placed first by our algorithm. This seems a bit strange as our mathematical problem is the same. Sometimes the linear algebra routines tries to normalize the equations to find the pivot element that would have been the largest element if all equations were normalized according to some rule, this is called *implicit pivoting*.  \n",
    "## LU decomposition\n",
    "As we have already seen, if the matrix $\\mathbf{A}$ is reduced to a triangular form it is trivial to calculate the solution by using backsubstitution. Thus if it was possible to decompose the matrix $\\mathbf{A}$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:lu\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}=\\mathbf{L}\\cdot\\mathbf{U}\\label{eq:lin:lu} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "&\\begin{pmatrix}\n",
    "a_{0,0}&a_{0,1}&a_{0,2}&a_{0,3}\\\\ \n",
    "a_{1,0}&a_{1,1}&a_{1,2}&a_{1,3}\\\\ \n",
    "a_{2,0}&a_{2,1}&a_{2,2}&a_{2,3}\\\\ \n",
    "a_{3,0}&a_{3,1}&a_{3,2}&a_{3,3}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "l_{0,0}&0&0&0\\\\ \n",
    "l_{1,0}&l_{1,1}&0&0\\\\ \n",
    "l_{2,0}&l_{2,1}&l_{2,2}&0\\\\ \n",
    "l_{3,0}&l_{3,1}&l_{3,2}&l_{3,3}\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "u_{0,0}&u_{0,1}&u_{0,2}&u_{0,3}\\\\ \n",
    "0&u_{1,1}&u_{1,2}&u_{1,3}\\\\ \n",
    "0&0&u_{2,2}&u_{2,3}\\\\ \n",
    "0&0&0&u_{3,3}\n",
    "\\end{pmatrix}.{\\nonumber}\n",
    "\\label{_auto3} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution procedure would then be to rewrite equation ([5](#eq:lin:mat)) as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:matb\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A\\cdot x}=\\mathbf{L}\\cdot\\mathbf{U}\\cdot\\mathbf{x}=\\mathbf{b},\\label{eq:lin:matb} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define a new vector $\\mathbf{y}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{y}\\equiv\\mathbf{U}\\cdot\\mathbf{x},\n",
    "\\label{_auto4} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can first solve for the $\\mathbf{y}$ vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:for\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{L}\\cdot\\mathbf{y}=\\mathbf{b},\\label{eq:lin:for} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then for $\\mathbf{x}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{U}\\cdot\\mathbf{x}=\\mathbf{y}.\n",
    "\\label{_auto5} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the solution to equation ([16](#eq:lin:for)) would be done by *forward substitution*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:back3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i=\\frac{1}{l_{ii}}\\left[b_i-\\sum_{j=0}^{i-1}l_{ij}x_j\\right],\\quad i=1,2,\\ldots N-1.\n",
    "\\label{eq:lin:back3} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why go to all this trouble? First of all it requires (slightly) less operations to calculate the LU decomposition and doing the forward and backward substitution than the Gauss-Jordan procedure discussed earlier. Secondly, and more importantly, is the fact that in many cases one would like to calculate the solution for different values of the $\\mathbf{b}$ vector in equation ([14](#eq:lin:matb)). If we do the LU decomposition first we can calculate the solution quite fast using backward and forward substitution for any value of the $\\mathbf{b}$ vector.\n",
    "\n",
    "The NumPy function [`solve`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html), uses LU decomposition and partial pivoting, and we can find the solution to our previous problem simply by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "x=solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative methods\n",
    "The methods described so far are what is called *direct* methods. The direct methods for very large systems might suffer from round off errors. That means that even if the computer has found a solution, the solution is \"polluted\" by round off errors, or stated more clearly: your solution for $\\mathbf{x}$, when entered into the original equation $\\mathbf{A}\\mathbf{x}\\neq\\mathbf{b}$. Below we will describe one trick, and two alternative methods to the direct methods.\n",
    "## Iterative improvement\n",
    "The first method [[press2001]](#press2001) assumes that we already have solved the matrix equation ([5](#eq:lin:mat)), and obtained an *estimate* $\\mathbf{\\hat{x}}$ of the true solution $\\mathbf{x}$. Assume that $\\mathbf{\\hat{x}}=\\mathbf{x}+\\delta\\mathbf{x}$, and that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:itb\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\mathbf{\\hat{x}}=\\mathbf{A}\\cdot(\\mathbf{x}+\\delta\\mathbf{x})=\\mathbf{b}+\\delta\\mathbf{b},\n",
    "\\label{eq:lin:itb} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subtracting equation ([5](#eq:lin:mat)) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:itb2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\delta\\mathbf{x}=\\delta\\mathbf{b}.\n",
    "\\label{eq:lin:itb2} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving equation ([19](#eq:lin:itb)) for $\\delta\\mathbf{b}$ an inserting in the equation above, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:itb3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\delta\\mathbf{x}=\\mathbf{A}\\cdot\\mathbf{\\hat{x}}-\\mathbf{b}.\n",
    "\\label{eq:lin:itb3} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usefulness of this method assumes that we have already obtained the LU decomposition of $\\mathbf{A}$, and if possible one should use a higher precision to calculate the right hand side, since there will be a lot of cancellations. Then the whole computational process it is simply to calculate the right hand side and backsubstitute. The improved solution is then obtained by subtracting $\\delta\\mathbf{x}$ from $\\mathbf{\\hat{x}}$.\n",
    "\n",
    "## The Jacobi method\n",
    "A completely different approach is the Jacobian method, which is simply to decompose the $\\mathbf{A}$ matrix in the following way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:DR\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}=\\mathbf{D}+\\mathbf{R}\\label{eq:lin:DR} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "&\\begin{pmatrix}\n",
    "a_{0,0}&a_{0,1}&a_{0,2}&a_{0,3}\\\\ \n",
    "a_{1,0}&a_{1,1}&a_{1,2}&a_{1,3}\\\\ \n",
    "a_{2,0}&a_{2,1}&a_{2,2}&a_{2,3}\\\\ \n",
    "a_{3,0}&a_{3,1}&a_{3,2}&a_{3,3}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a_{0,0}&0&0&0\\\\ \n",
    "0&a_{1,1}&0&0\\\\ \n",
    "0&0&a_{2,2}&0\\\\ \n",
    "0&0&0&a_{3,3}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "&\\begin{pmatrix}\n",
    "0&a_{0,1}&a_{0,2}&a_{0,3}\\\\ \n",
    "a_{1,0}&0&a_{1,2}&a_{1,3}\\\\ \n",
    "a_{2,0}&a_{2,1}&0&a_{2,3}\\\\ \n",
    "a_{3,0}&a_{3,1}&0&a_{3,3}\n",
    "\\end{pmatrix}.{\\nonumber}\n",
    "\\label{_auto6} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write equation ([5](#eq:lin:mat)) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{D}\\mathbf{x}=\\mathbf{b}-\\mathbf{R}\\cdot\\mathbf{x}.\n",
    "\\label{eq:lin:jc} \\tag{24}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this help us? First of all, the matrix $\\mathbf{D}$ is easy to invert as it is diagonal, the inverse can be found by simply replace $a_{ii}\\to 1/a_{ii}$. But $\\mathbf{x}$ is still present on the right hand side? This is where the *iterations* comes into play, we simply guess at an initial solution $\\mathbf{x}^k$, and then we use equation ([24](#eq:lin:jc)) to calculate the next solution $\\mathbf{x}^{k+1}$, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{k+1}=\\mathbf{D}^{-1}(\\mathbf{b}-\\mathbf{R}\\cdot\\mathbf{x}^{k}).\n",
    "\\label{eq:lin:jc2} \\tag{25}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write it out on component form for a $4\\times4$ matrix to see what is going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc3a\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_0 =\\frac{1}{a_{00}}(b_0-a_{01}x_1^k-a_{02}x_2^k-a_{03}x_3^k),\\label{eq:lin:jc3a} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc3b\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_1 =\\frac{1}{a_{11}}(b_0-a_{00}x_0^k-a_{02}x_2^k-a_{03}x_3^k),\\label{eq:lin:jc3b} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc3c\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_2 =\\frac{1}{a_{22}}(b_0-a_{00}x_0^k-a_{01}x_1^k-a_{03}x_3^k),\\label{eq:lin:jc3c} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:jc3d\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_3 =\\frac{1}{a_{33}}(b_0-a_{00}x_0^k-a_{01}x_1^k-a_{02}x_2^k).\\label{eq:lin:jc3d} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def solve_jacobi(A,b,x=np.zeros(len(b)),max_iter=1000,EPS=1e-6):\n",
    "    \"\"\"\n",
    "    Solves the linear system Ax=b using the jacobi method, stops if\n",
    "    solution is not found after max_iter or if solution changes less \n",
    "    than EPS\n",
    "    \"\"\"\n",
    "    D=np.diag(A)\n",
    "    R=A-np.diag(D)\n",
    "    eps=1\n",
    "    x_old=x\n",
    "    iter=0\n",
    "    while(eps>EPS and iter<max_iter):\n",
    "        iter+=1\n",
    "        x=(b-np.dot(R,x_old))/D\n",
    "        eps=np.abs(np.sum(x-x_old))\n",
    "        x_old=x\n",
    "    print('found solution after ' + str(iter) +' iterations')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative method can be appealing if we do not need a high accuracy, we can choose to stop whenever $|\\mathbf{x}^{k+1}-\\mathbf{x}^k|$ is small enough. For the direct method we have to follow through all the way.\n",
    "**Convergence.**\n",
    "\n",
    "The Jacobi method converges if the matrix $\\mathbf{A}$ is strictly diagonally dominant. Strictly diagonally dominant means that the absolute value of each entry on the diagonal is greater than the sum of the absolute values of the other entries in the same row, i.e if $|a_{00}|>|a_{01}+a_{02}+\\cdots|$. In general it can be shown that a iterative scheme $\\mathbf{x}^{k+1}=\\mathbf{P}\\cdot \\mathbf{x}^k+\\mathbf{q}$ is convergent *if and only if* every eigenvalue, $\\lambda$, of $\\mathbf{P}$ satisfies $|\\lambda|<1$, i.e. the *spectral radius* $\\rho(\\mathbf{P})<1$.\n",
    "\n",
    "\n",
    "\n",
    "## The Gauss-Seidel method\n",
    "It is tempting in equation ([26](#eq:lin:jc3a)) to use our estimate of $x_0^{k+1}$ in the next equation, equation ([27](#eq:lin:jc3b)), instead of $x_0^k$. After all our estimate $x_0^{k+1}$ is an *improved* estimate. This is actually the Gauss-Seidel method. This method also has the advantage that if there are memory issues, one can overwrite the old value of $x_i^k$. Usually the Gauss-Seidel method converges faster, but not always. A plus for the Jacobi method is that is can be  parallelised, as the calculations is only dependent on the old values and do not require information about the new values as for the Gauss Seidel method. Below is a Python implementation of the Gauss-Seidel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def solve_GS(A,b,x=np.zeros(len(b)),max_iter=1000,EPS=1e-6):\n",
    "    \"\"\"\n",
    "    Solves the linear system Ax=b using the Gauss-Seidel method, stops if\n",
    "    solution is not found after max_iter or if solution changes less \n",
    "    than EPS\n",
    "    \"\"\"\n",
    "    D=np.diag(A)\n",
    "    R=A-np.diag(D)\n",
    "    eps=1\n",
    "    iter=0\n",
    "    while(eps>EPS and iter<max_iter):\n",
    "        iter+=1\n",
    "        eps=0.\n",
    "        for i in range(len(x)):\n",
    "            tmp=x[i]\n",
    "            x[i]=w*(b[i]- np.dot(R[i,:],x))/D[i]\n",
    "            eps+=np.abs(tmp-x[i])\n",
    "    print('found solution after ' + str(iter) +' iterations')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Linear regression\n",
    "In the previous section, we considered a system of $N$ equations and $N$ unknown ($x_0, x_1,\\ldots, x_N$). In general we might have more equations than unknowns or more unknowns than equations. An example of the former is linear regression, we might have many data points and we would like to fit a line through the points. How do you fit a single lines to more than two points that does not line on the same line? One way to do it is to minimize the distance from the line to the points, as illustrated in [figure](#fig:lin:reg).\n",
    "<!-- dom:FIGURE: [fig-lin/reg.png, width=800 frac=.9] Linear regression by minimizing the total distance to all the points. <div id=\"fig:lin:reg\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:lin:reg\"></div>\n",
    "\n",
    "<p>Linear regression by minimizing the total distance to all the points.</p>\n",
    "<img src=\"fig-lin/reg.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "Mathematically we can express the distance between a data point $(x_i,y_i)$ and the line $f(x)$ as $y_i-f(x_i)$. Note that this difference can be negative or positive depending if the data point lies below or above the line. We can then take the absolute value of all the distances, and try to minimize them. When we minimize something we take the derivative of the expression and put it equal to zero.  As you might remember from Calculus it is extremely hard to work with the derivative of the absolute value, because it is discontinuous. A much better approach is to square each distance and sum them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:lsq\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S=\\sum_{i=0}^{N-1}(y_i-f(x_i))^2=\\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)^2.\n",
    "\\label{eq:lin:lsq} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the example in [figure](#fig:lin:reg), $N=5$.) This is the idea behind *least square*, and linear regression. One thing you should be aware of is that points lying far from the line will contribute more to equation ([30](#eq:lin:lsq)). The underlying assumption is that each data point provides equally precise information about the process, this is often not the case. When analyzing experimental data, there may be points deviating from the expected behaviour, it is then important to investigate if these points are more affected by measurements errors than the others. If that is the case one should give them less weight in the least square estimate, by extending the formula above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:lsqm\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S=\\sum_{i=0}^{N-1}\\omega_i(y_i-f(x_i))^2=\\sum_{i=0}^3\\omega_i(y_i-a_0-a_1x_i)^2,\n",
    "\\label{eq:lin:lsqm} \\tag{31}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\omega_i$ is a weight factor.\n",
    "\n",
    "## Solving least square, using algebraic equations\n",
    "Let us continue with equation ([30](#eq:lin:lsq)), the algebraic solution is to simply find the value of $a_0$ and $a_1$ that minimizes $S$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial S}{\\partial a_0} =-2\\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)=0,\n",
    "\\label{eq:lin:ls1} \\tag{32} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{\\partial S}{\\partial a_1} =-2\\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)x_i=0.\n",
    "\\label{eq:lin:ls2} \\tag{33}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the mean value as $\\overline{x}=\\sum_ix_i/N$ and $\\overline{y}=\\sum_iy_i/N$, we can write equation ([32](#eq:lin:ls1)) and ([33](#eq:lin:ls2))  as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls1a\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)=N\\overline{y}-a_0N-a_1N\\overline{x}=0,\n",
    "\\label{eq:lin:ls1a} \\tag{34} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls2b\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\sum_{i=0}^{N-1}(y_i-a_0-a_1x_i)x_i=\\sum_iy_ix_i-a_0N\\overline{x}-a_1\\sum_ix_ix_i=0.\n",
    "\\label{eq:lin:ls2b} \\tag{35}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving equation ([34](#eq:lin:ls1a)) with respect to $a_0$, and inserting the expression into equation ([35](#eq:lin:ls2b)), we find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls1c\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a_0=\\overline{y}-a_1\\overline{x},\\label{eq:lin:ls1c} \\tag{36} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:ls2d\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "a_1=\\frac{\\sum_iy_ix_i-N\\overline{x}\\overline{y}}{\\sum_ix_i^2-N\\overline{x}^2}\n",
    "=\\frac{\\sum_i(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_i(x_i-\\overline{x})^2}.\n",
    "\\label{eq:lin:ls2d} \\tag{37}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it as an exercise to show the last expression for $a_1$.  \n",
    "Clearly the equation ([37](#eq:lin:ls2d)) above will in most cases have\n",
    "a solution. But in addition to a solution, it would be good to have an\n",
    "idea of the goodness of the fit. Intuitively it make sense to add all\n",
    "the distances (residuals) $d_i$ in [figure](#fig:lin:reg). This is\n",
    "basically what is done when calculating $R^2$ (R-squared). However, we\n",
    "would also like to compare the $R^2$ between different\n",
    "datasets. Therefor we need to normalize the sum of residuals, and\n",
    "therefore the following form of the $R^2$ is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:r2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "R^2=1-\\frac{\\sum_{i=0}^{N-1}(y_i-f(x_i))^2}{\\sum_{i=0}^{N-1}(y_i-\\overline{y})^2}.\n",
    "\\label{eq:lin:r2} \\tag{38}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python we can implement equation ([36](#eq:lin:ls1c)), ([37](#eq:lin:ls2d)) and ([38](#eq:lin:r2)) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def OLS(x, y): \n",
    "    # returns regression coefficients\n",
    "    # in ordinary least square\n",
    "    # x: observations\n",
    "    # y: response\n",
    "    # R^2: R-squared\n",
    "    n = np.size(x) # number of data points \n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    b_1 = SS_xy / SS_xx \n",
    "    b_0 = m_y - b_1*m_x\n",
    "\n",
    "    #R^2\n",
    "    y_pred = b_0 + b_1*x\n",
    "    S_yy   = np.sum(y*y) - n*m_y*m_y\n",
    "    y_res  = y-y_pred  \n",
    "    S_res  = np.sum(y_res*y_res)\n",
    "  \n",
    "    return(b_0, b_1,1-S_res/S_yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square as a linear algebra problem\n",
    "It turns out that the least square problem can be formulated as a\n",
    "matrix problem. (Two great explanations see [linear regression by\n",
    "matrices](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b),\n",
    "and\n",
    "[$R^2$-squared](https://medium.com/@andrew.chamberlain/a-more-elegant-view-of-r-squared-a0a14c177dc3).)\n",
    "If we define a matrix $\\mathbf{X}$ containing the observations $x_i$\n",
    "as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:mreg1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "1&x_0\\\\ \n",
    "1&x_1\\\\ \n",
    "\\vdots&\\vdots\\\\ \n",
    "1&x_{N-1}\n",
    "\\end{pmatrix}.\n",
    "\\label{eq:lin:mreg1} \\tag{39}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a vector containing all the response $\\mathbf{y}$, and the\n",
    "regression coefficients $\\mathbf{a}=(a_0,a_1)$. Then we can write\n",
    "equation ([31](#eq:lin:lsqm)) as a matrix equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:mregS\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S=(\\mathbf{y}-\\mathbf{X\\cdot a})^T(\\mathbf{y}-\\mathbf{X\\cdot a}).\n",
    "\\label{eq:lin:mregS} \\tag{40}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that this equation can easily be extended to more than one\n",
    "observation variable $x_i$*. By simply differentiating equation\n",
    "([40](#eq:lin:mregS)) with respect to $\\mathbf{a}$, we can show that\n",
    "the derivative has a minimum when (see proof below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:mregS2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^T\\mathbf{X a}=\\mathbf{X}^T\\mathbf{y}\n",
    "\\label{eq:lin:mregS2} \\tag{41}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python implementation of equation ([41](#eq:lin:mregS2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def OLSM(x, y): \n",
    "    # returns regression coefficients\n",
    "    # in ordinary least square using solve function\n",
    "    # x: observations\n",
    "    # y: response\n",
    "\n",
    "    XT = np.array([np.ones(len(x)),x],float)\n",
    "    X  = np.transpose(XT)\n",
    "    B = np.dot(XT,X)\n",
    "    C = np.dot(XT,y)\n",
    "    return solve(B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with matrices on component form\n",
    "Whenever you want to do some manipulation with matrices, it is very useful to simply write them on component form. If we multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$ to form a new matrix $\\mathbf{C}$, the components of the new matrix is simply $\\mathbf{C}_{ij}=\\sum_k\\mathbf{A}_{ik}\\mathbf{B}_{kj}$. The strength of doing this is that the elements of a matrix, e.g. $\\mathbf{A}_{ik}$ are *numbers*, and we can move them around. Proving that e.g. $(\\mathbf{A}\\mathbf{B})^T=\\mathbf{B}^T\\mathbf{A}^T$ is straight forward using the component form. The transpose of a matrix is simply to exchange columns and rows, hence $\\mathbf{C}_{ij}^T=\\mathbf{C}_{ji}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:trans\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{C}_{ij}^T=\\mathbf{C}_{ji}=\\sum_k\\mathbf{A}_{jk}\\mathbf{B}_{ki}=\\sum_k\\mathbf{B}^T_{ik}\\mathbf{A}^T_{kj}\n",
    "=(\\mathbf{B}^T\\mathbf{A}^T)_{ij},\n",
    "\\label{eq:lin:trans} \\tag{42}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus $\\mathbf{C}^T=\\mathbf{B}^T\\mathbf{A}^T$. To derive equation ([41](#eq:lin:mregS2)), we need to take the derivative of equation ([41](#eq:lin:mregS2)) with respect to $\\mathbf{a}$.\n",
    "What we mean by this is that we want to evaluate $\\partial S/\\partial a_k$ for all the components of $\\mathbf{a}$.\n",
    "A useful rule is $\\partial a_i/\\partial a_k=\\delta_{ik}$, where $\\delta_{ik}$ is the Kronecker delta, it takes the value of one if $i=k$ and zero otherwise. We can write $S=\\mathbf{y}^T\\mathbf{y}-\\mathbf{y}\\mathbf{X\\cdot a}\n",
    "-(\\mathbf{X\\cdot a})^T\\mathbf{y}-(\\mathbf{X\\cdot a})^T\\mathbf{X\\cdot a}$. All terms that do not contain $\\mathbf{a}$ are zero, thus we only need to evaluate the following terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{a_k}(\\mathbf{X\\cdot a})^T\\mathbf{y} =\\frac{\\partial}{a_k}(\\mathbf{a}^T\\cdot \\mathbf{X}^T\\mathbf{y})=\\frac{\\partial}{a_k}\\sum_{ij}\\mathbf{a}^T_i\\mathbf{X}^T_{ij}\\mathbf{y}_j\n",
    "=\\sum_{ij}\\delta_{ik}\\mathbf{X}^T_{ij}\\mathbf{y}_j{\\nonumber}\n",
    "\\label{_auto7} \\tag{43}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "=\\sum_{j}\\mathbf{X}^T_{kj}\\mathbf{y}_j=\\mathbf{X}^T\\mathbf{y} \n",
    "\\label{_auto8} \\tag{44}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{\\partial}{a_k}\\mathbf{y}^T\\mathbf{X\\cdot a}=\\frac{\\partial}{a_k}\\sum_{ij}\\mathbf{y}^T_i\\mathbf{X}_{ij}\\mathbf{a}_j\n",
    "=\\sum_{ij}\\mathbf{y}^T_i\\mathbf{X}_{ij}\\delta_{jk}=\\sum_{j}\\mathbf{y}^T_{i}\\mathbf{X}_{ik}{\\nonumber}\n",
    "\\label{_auto9} \\tag{45}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "=\\sum_{j}\\mathbf{y}^T_{i}\\mathbf{X}^T_{ki}=\\mathbf{X}^T\\mathbf{y} \n",
    "\\label{_auto10} \\tag{46}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{\\partial}{a_k} (\\mathbf{X\\cdot a})^T\\mathbf{X\\cdot a}=\n",
    "\\frac{\\partial}{a_k}\\sum_{ijl} \\mathbf{a}^T_i\\mathbf{X}^T_{ij}\\mathbf{X}_{jl}\\mathbf{a}_l=\n",
    "\\sum_{ijl}(\\delta_{ik}\\mathbf{X}^T_{ij}\\mathbf{X}_{jl}\\mathbf{a}_l+\\mathbf{a}^T_i\\mathbf{X}^T_{ij}\\mathbf{X}_{jl}\\delta_{lk}){\\nonumber}\n",
    "\\label{_auto11} \\tag{47}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} =\\sum_{jl}\\mathbf{X}^T_{kj}\\mathbf{X}_{jl}\n",
    "\\mathbf{a}_l+\\sum_{ij}\\mathbf{a}^T_i\\mathbf{X}^T_{ij}\\mathbf{X}_{jk}{\\nonumber}\n",
    "\\label{_auto12} \\tag{48}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}  \n",
    "=\\mathbf{X}^T\\mathbf{X}\\mathbf{a}+\\sum_{ij}\\mathbf{X}^T_{kj}\\mathbf{X}_{ji}\\mathbf{a}_i\n",
    "= 2\\mathbf{X}^T\\mathbf{X}\\mathbf{a}.\n",
    "\\label{}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then follows that $\\partial S/\\partial \\mathbf{a} = 0$ when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:matpr\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^T\\mathbf{X a}=\\mathbf{X}^T\\mathbf{y}.\n",
    "\\label{eq:lin:matpr} \\tag{49}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrices and Thomas algorithm\n",
    "In many practical examples, such as solving partial differential\n",
    "equations the matrices could be quite large and also contain a lot of\n",
    "zeros. A very important class of such matrices are *banded matrices*\n",
    "this is a type of *sparse matrices* containing a lot of zero elements,\n",
    "and the non-zero elements are confined to diagonal bands. In the\n",
    "following we will focus on one important type of sparse matrix the\n",
    "tridiagonal. In the next section we will show how it enters naturally\n",
    "in solving the heat equation. It turns out that solving banded\n",
    "matrices is quite simple, and can be coded quite efficiently. As with\n",
    "the Gauss-Jordan example, lets consider a concrete example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\begin{array}{ccccc|c}\n",
    "b_0&c_0&0&0&0&r_0\\\\ \n",
    "a_1&b_1&c_1&0&0&r_1\\\\ \n",
    "0&a_2&b_2&c_2&0&r_2\\\\ \n",
    "0& 0&a_3&b_3&c_3&r_3\\\\ \n",
    "0& 0& 0&a_4&b_4&r_4\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\label{_auto13} \\tag{50}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right hand side is represented with $r_i$. The first Gauss-Jordan\n",
    "step is simply to divide by $b_0$, then we multiply with $-a_1$ and\n",
    "add to second row:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto14\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\to \\left(\n",
    "\\begin{array}{ccccc|c}\n",
    "1&c_0^\\prime&0&0&0&r_0^\\prime\\\\ \n",
    "0&b_1-a_1c_0^\\prime&c_1&0&0&r_1-a_0r_0^\\prime\\\\ \n",
    "0&a_2&b_2&c_2&0&r_2\\\\ \n",
    "0& 0&a_3&b_3&c_3&r_3\\\\ \n",
    "0& 0& 0&a_4&b_4&r_4\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\label{_auto14} \\tag{51}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have introduced some new symbols to simplify the\n",
    "notation: $c_0^\\prime=c_0/b_0$ and $r_0^\\prime=r_0/b_0$. Then we\n",
    "divide by $b_1-a_1c_0^\\prime$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto15\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\begin{array}{ccccc|c}\n",
    "1&c_0^\\prime&0&0&0&r_0^\\prime\\\\ \n",
    "0&1&c_1^\\prime&0&0&r_1^\\prime\\\\ \n",
    "0&a_2&b_2&c_2&0&r_2\\\\ \n",
    "0& 0&a_3&b_3&c_3&r_3\\\\ \n",
    "0& 0& 0&a_4&b_4&r_4\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\label{_auto15} \\tag{52}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $c_1^\\prime=c_1/(b_1-a_1c_0^\\prime)$ and\n",
    "$r_1^\\prime=(r_1-a_0r_0^\\prime)/(b_1-a_1c_0^\\prime)$. If you continue\n",
    "in this manner, you can easily convince yourself that to transform a\n",
    "tridiagonal matrix to the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto16\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\to \\left(\n",
    "\\begin{array}{ccccc|c}\n",
    "1&c_0^\\prime&0&0&0&r_0^\\prime\\\\ \n",
    "0&1&c_1^\\prime&0&0&r_1^\\prime\\\\ \n",
    "0&0&1&c_2^\\prime&0&r_2^\\prime\\\\ \n",
    "0& 0&0&1&c_3^\\prime&r_3^\\prime\\\\ \n",
    "0& 0& 0&0&1&r_4^\\prime\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\label{_auto16} \\tag{53}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:th0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_0^\\prime =\\frac{c_0}{b_0} \\qquad r_0^\\prime={r_0}{b_0}\n",
    "\\label{eq:lin:th0} \\tag{54} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:thi\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "c_i^\\prime\n",
    "=\\frac{c_i}{b_i-a_ic_{i-1}^\\prime}\\qquad\n",
    "r_i^\\prime=\\frac{r_i-a_ir_{i-1}^\\prime}{b_i-a_ic_{i-1}^\\prime}\n",
    "\\quad\\text{, for }i=1,2,\\ldots,N-1\\label{eq:lin:thi} \\tag{55} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we where able to reduce the tridiagonal matrix to an *upper\n",
    "triangular* matrix in only *one* Gauss-Jordan step. This equation can\n",
    "readily be solved using back-substitution, which can also be\n",
    "simplified as there are a lot of zeros in the upper part. Let us\n",
    "denote the unknowns $x_i$ as we did for the Gauss-Jordan case, now we\n",
    "can find the solution as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:this0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{N-1}  = r_{N-1}^\\prime \\label{eq:lin:this0} \\tag{56} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:thisi\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "x_i      = r_i^\\prime-x_{i+1}c_i^\\prime\\quad\\text{, for } i=N-2,N-3,\\ldots,0\n",
    "\\label{eq:lin:thisi} \\tag{57}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation ([54](#eq:lin:th0)), ([55](#eq:lin:thi)), ([56](#eq:lin:this0))\n",
    "and ([57](#eq:lin:thisi)) is known as the Thomas algorithm after\n",
    "Llewellyn Thomas. \n",
    "**Notice.**\n",
    "\n",
    "Clearly tridiagonal matrices can be solved much more efficiently with\n",
    "the Thomas algorithm than\n",
    "using a standard library, such as LU-decomposition. This is\n",
    "because the solution method takes advantages of the *symmetry* of the\n",
    "problem. We will not show it here, but it can be shown that the Thomas\n",
    "algorithm is stable whenever $|b_i|\\ge |a_i|+|c_i|$. If the algorithm\n",
    "fails, an advice is first to use the standard `solve` function in\n",
    "python. If this gives a solution, then *pivoting* combined with the\n",
    "Thomas algorithm might do the trick.\n",
    "\n",
    "\n",
    "# Example: Solving the heat equation using linear algebra\n",
    "\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 1: Conservation Equation or the Continuity Equation\n",
    "\n",
    "<!-- dom:FIGURE: [fig-lin/heat.png, width=700 frac=.9] Conservation of energy and the continuity equation. <div id=\"fig:nlin:heat\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:heat\"></div>\n",
    "\n",
    "<p>Conservation of energy and the continuity equation.</p>\n",
    "<img src=\"fig-lin/heat.png\" width=700>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "In [figure](#fig:nlin:heat), the continuity equation is derived for\n",
    "heat flow.\n",
    "### Heat equation for solids\n",
    "\n",
    "Derive the heat equation for a solid and show that it can be written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heateq\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d^2T}{dx^2}+\\frac{\\dot{\\sigma}}{k}=\\frac{\\rho c_p}{k}\\frac{dT}{dt},\n",
    "\\label{eq:nlin:heateq} \\tag{58}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\dot{\\sigma}$ is the rate of heat generation in the solid. This\n",
    "equation can be used as a starting point for many interesting\n",
    "models. In this exercise we will investigate the *steady state*\n",
    "solution, *steady state* is just a fancy way of expressing that we\n",
    "want the solution that *does not change with time*. This is achieved\n",
    "by ignoring the derivative with respect to time in equation\n",
    "([58](#eq:nlin:heateq)). We want to study a system with size $L$, and is\n",
    "it good practice to introduce a dimensionless variable: $y=x/L$. \n",
    "Show that equation ([58](#eq:nlin:heateq)) now takes the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heat2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d^2T }{dy^2}+\\frac{\\dot{\\sigma}L^2}{k}=0\n",
    "\\label{eq:nlin:heat2} \\tag{59}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --- end exercise --- -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 2: Curing of Concrete and Matrix Formulation\n",
    "\n",
    "Curing of concrete is one particular example that we can investigate\n",
    "with equation ([59](#eq:nlin:heat2)). When concrete is curing, there are\n",
    "a lot of chemical reactions happening, these reactions generate\n",
    "heat. This is a known issue, and if the temperature rises too much \n",
    "compared to the surroundings, the concrete may fracture.  In the\n",
    "following we will, for simplicity, assume that the rate of heat\n",
    "generated during curing is constant, $\\dot{\\sigma}=$100 W/m$^3$. The\n",
    "left end (at $x=0$) is insulated, meaning that there is no flow of\n",
    "heat over that boundary, hence $dT/dx=0$ at $x=0$. On the right hand\n",
    "side the temperature is kept constant, $x(L)=y(1)=T_1$, assumed to be\n",
    "equal to the ambient temperature of $T_1=25^\\circ$C.  The concrete\n",
    "thermal conductivity is assumed to be $k=1.65$ W/m$^\\circ$C.\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "\n",
    "Show that the solution to equation ([59](#eq:nlin:heat2)) in this case is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heatsol\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(y)=\\frac{\\dot{\\sigma}L^2}{2k}(1-y^2)+T_1.\n",
    "\\label{eq:nlin:heatsol} \\tag{60}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve equation ([59](#eq:nlin:heat2)) numerically, we need to discretize\n",
    "it.\n",
    "\n",
    "**Part 2.**\n",
    "\n",
    "Show that equation ([59](#eq:nlin:heat2)) now takes the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heat3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T_{i+1}+T_{i-1}-2T_i=-h^2\\beta,\n",
    "\\label{eq:nlin:heat3} \\tag{61}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\beta=\\dot{\\sigma}L^2/k$.\n",
    "<!-- dom:FIGURE: [fig-lin/heat_grid.png, width=200 frac=.5] Finite difference grid for $N=4$. <div id=\"fig:nlin:hgrid\"></div>  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:nlin:hgrid\"></div>\n",
    "\n",
    "<p>Finite difference grid for $N=4$.</p>\n",
    "<img src=\"fig-lin/heat_grid.png\" width=200>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "In [figure](#fig:nlin:hgrid), the finite difference grid is shown for\n",
    "$N=4$. Let us write down equation ([61](#eq:nlin:heat3)) for each grid\n",
    "node to see how the implementation is done in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto17\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T_{-1}+T_1-2T_0 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto17} \\tag{62}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto18\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{0}+T_2-2T_1 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto18} \\tag{63}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto19\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{1}+T_3-2T_2 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto19} \\tag{64}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto20\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{2}+T_4-2T_3 =-h^2\\beta.{\\nonumber}\n",
    "\\label{_auto20} \\tag{65}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heat4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\label{eq:nlin:heat4} \\tag{66}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part is now to introduce the boundary conditions. The right\n",
    "hand side is easy, because here the temperature is $T_4=25$. However,\n",
    "we see that $T_{-1}$ enters and we have no value for this node. The\n",
    "boundary condition on the left hand side is $dT/dy=0$, by using the\n",
    "central difference for the derivative allows us to write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:bound1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left.\\frac{dT}{dy}\\right|_{y=0}=\\frac{T_{-1}-T_1}{2h}=0,\n",
    "\\label{eq:nlin:bound1} \\tag{67}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence $T_{-1}=T_1$. Thus the final set of equations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto21\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "2T_1-2T_0 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto21} \\tag{68}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto22\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{0}+T_2-2T_1 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto22} \\tag{69}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto23\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{1}+T_3-2T_2 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto23} \\tag{70}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto24\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "T_{2}+25-2T_3 =-h^2\\beta,{\\nonumber}\n",
    "\\label{_auto24} \\tag{71}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:nlin:heat5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\label{eq:nlin:heat5} \\tag{72}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in matrix form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lin:heats\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "-2&2&0&0\\\\ \n",
    "1&-2&1&0\\\\ \n",
    "0&1&-2&1\\\\ \n",
    "0&0&1&-2\\\\ \n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "T_0\\\\ \n",
    "T_1\\\\ \n",
    "T_2\\\\ \n",
    "T_3\\\\ \n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "-h^2\\beta\\\\ \n",
    "-h^2\\beta\\\\ \n",
    "-h^2\\beta\\\\ \n",
    "-h^2\\beta-25\\\\ \n",
    "\\end{array}\n",
    "\\right).\n",
    "\\end{equation}\n",
    "\\label{eq:lin:heats} \\tag{73}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is now easy to increase $N$ as it is only the boundaries\n",
    "that requires special attention.\n",
    "\n",
    "* Solve the set of equations using [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html).\n",
    "\n",
    "The correct solution is $L=1$ m, and $h=1/4$, is: $[T_0,T_1.T_2,T_3]=[55.3030303 , 53.40909091, 47.72727273, 38.25757576]$.\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 3: Using sparse matrices in python\n",
    "\n",
    "In this part we are going to create a sparse matrix in python and use `scipy.sparse.linalg.spsolve` to solve it. The matrix is created using `scipy.sparse.spdiags`.\n",
    "\n",
    "**Part 3.**\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import scipy.sparse.linalg\n",
    "# Set simulation parameters\n",
    "\n",
    "h = 0.25                # step size\n",
    "L = 1.0                 # domain size \n",
    "n = int(round(L/h)) -1  # number of nodes\n",
    "\n",
    "beta = ....\n",
    "\n",
    "def analytical(beta,x):\n",
    "    return ....\n",
    "#Set up sparse matrix\n",
    "diagonals=np.zeros((3,n))\n",
    "diagonals[0,:]= ...                       \n",
    "diagonals[1,:]= ... \n",
    "diagonals[2,:]= ...\n",
    "# make sure to set up correct boundary conditions!\n",
    "A_sparse = sc.sparse.spdiags(..., ..., , n, n,format='csc') \n",
    "\n",
    "#rhs array here:\n",
    "d = ...\n",
    "\n",
    "T = sc.sparse.linalg.spsolve( ... )\n",
    "\n",
    "# if you like you can use timeit to check the efficiency\n",
    "# %timeit sc.sparse.linalg.spsolve( ... )\n",
    "\n",
    "# make a plot, that compares the analytical result and the numerical, test for varying degree of step size h\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How does your solution compare with the analytical results, where is\n",
    "  the match good?\n",
    "\n",
    "* What step size do you need in order to get a good match?\n",
    "\n",
    "* Compare the sparse solver with the standard Numpy solver using\n",
    "  `%timeit`, how large must the linear system be before an improvement\n",
    "  in speed is seen?\n",
    "\n",
    "* How can we improve the numerical algorithm to get a better match?\n",
    "\n",
    "* Do you think the solution to this equation has practical implications? What are the limitations?\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n",
    "**Notice.**\n",
    "\n",
    "The solution below implements equation ([73](#eq:lin:heats)) using sparse matrices, and the standard Numpy `solve` function. You can use the `%timeit` magic command in Ipython and Jupyter notebooks to test the efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import scipy.sparse.linalg\n",
    "from numpy.linalg import solve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set simulation parameters\n",
    "h = 0.25             # element size\n",
    "L = 1.0              # length of domain\n",
    "n = int(round(L/h))  # number of unknowns \n",
    "x=np.arange(n+1)*h   # includes right bc \n",
    "T1=25\n",
    "sigma = 100*L**2/1.65\n",
    "\n",
    "def tri_diag(a, b, c, k1=-1, k2=0, k3=1):\n",
    "    \"\"\" a,b,c diagonal terms \"\"\"\n",
    "    return np.diag(a, k1) + np.diag(b, k2) + np.diag(c, k3)\n",
    "\n",
    "def analytical(sigma,x):\n",
    "    return sigma*(1-x*x)/2+T1\n",
    "\n",
    "#Create matrix for linalg solver\n",
    "a=np.ones(n-1)\n",
    "b=-np.ones(n)*2\n",
    "c=np.ones(n-1)\n",
    "#Create matrix for sparse solver\n",
    "diagonals=np.zeros((3,n))\n",
    "diagonals[0,:]= 1\n",
    "diagonals[1,:]= -2  \n",
    "diagonals[2,:]= 1\n",
    "\n",
    "# rhs vector\n",
    "d=np.repeat(-h*h*sigma,n)\n",
    "\n",
    "#----boundary conditions ------\n",
    "#lhs - no flux of heat\n",
    "diagonals[2,1]= 2\n",
    "c[0]=2\n",
    "#rhs - constant temperature\n",
    "d[n-1]=d[n-1]-T1\n",
    "#------------------------------\n",
    "\n",
    "A=tri_diag(a,b,c)\n",
    "A_sparse = sc.sparse.spdiags(diagonals, [-1,0,1], n, n,format='csc') \n",
    "# to view matrix\n",
    "print(A_sparse.todense())\n",
    "#Solve linear problems\n",
    "Ta = solve(A,d,)\n",
    "Tb = sc.sparse.linalg.spsolve(A_sparse,d)\n",
    "#Add right boundary node\n",
    "Ta=np.append(Ta,T1)\n",
    "Tb=np.append(Tb,T1)\n",
    "#uncomment to test efficiency\n",
    "#%timeit sc.sparse.linalg.spsolve(A_sparse,d)\n",
    "#%timeit solve(A,d,)\n",
    "\n",
    "# Plot solutions\n",
    "plt.plot(x,Ta,x,Tb,'-.',x,analytical(sigma,x),':', lw=3)\n",
    "plt.xlabel(\"Dimensionless length\")\n",
    "plt.ylabel(r\"Temperature [$^\\circ$C]\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(T1-1)\n",
    "plt.legend(['sparse','linalg','analytical'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "1. <div id=\"press2007\"></div> **W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery**. \n",
    "    *Numerical Recipes 3rd Edition: the Art of Scientific Computing*,\n",
    "    3 edition,\n",
    "    Cambridge University Press,\n",
    "    2007.\n",
    "\n",
    "2. <div id=\"trefethen1997\"></div> **L. N. Trefethen and D. B. III**. \n",
    "    *Numerical Linear Algebra*,\n",
    "    SIAM,\n",
    "    1997.\n",
    "\n",
    "3. <div id=\"stoer2013\"></div> **J. Stoer and R. Bulirsch**. \n",
    "    *Introduction to Numerical Analysis*,\n",
    "    Springer Science & Business Media,\n",
    "    2013.\n",
    "\n",
    "4. <div id=\"strang2019\"></div> **G. Strang**. \n",
    "    *Linear Algebra and Learning From Data*,\n",
    "    Wellesley-Cambridge Press,\n",
    "    2019.\n",
    "\n",
    "5. <div id=\"press2001\"></div> **W. H. Press, W. T. Vetterling, S. A. Teukolsky and B. P. Flannery**. \n",
    "    *Numerical Recipes in C++: the Art of Scientific Computing*,\n",
    "    2nd edition,\n",
    "    Cambridge University Press,\n",
    "    2002."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
