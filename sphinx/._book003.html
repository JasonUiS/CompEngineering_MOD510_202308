
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="1.0">
  <head>
    <meta charset="utf-8" />
    <title>Finite differences &#8212; _ Aksel Hiorth, the National IOR Centre &amp; Institute for Energy Resources, University of Stavanger documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Solving linear systems" href="._book004.html" />
    <link rel="prev" title="Writing Python code" href="._book002.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  
       <style type="text/css">
         div.admonition {
           background-color: whiteSmoke;
           border: 1px solid #bababa;
         }
       </style>
      </head>
    <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="finite-differences">
<span id="ch-taylor"></span><h1>Finite differences<a class="headerlink" href="#finite-differences" title="Permalink to this headline">¶</a></h1>
<p>The mathematics introduced in this chapter is absolutely essential in order to understand the development of numerical algorithms. We strongly advice you to study it carefully, implement python scripts and investigate the results, reproduce the analytical derivations and compare with the numerical solutions.</p>
<div class="section" id="numerical-errors">
<h2>Numerical Errors<a class="headerlink" href="#numerical-errors" title="Permalink to this headline">¶</a></h2>
<p>To simulate a physical system in a computer model, we usually have to make space and time discrete. In order to simulate e.g. a rocket flying into space we typically find the position of the rocket at a specific time <span class="math notranslate nohighlight">\(t\)</span>, and the computer model calculates the new position at a later time <span class="math notranslate nohighlight">\(t+h\)</span>. <span class="math notranslate nohighlight">\(h\)</span> is a step size, and if we assume it is one minute, then we have discretized one hour into 60 discrete chunks of time. The challenge for any modeler is to know if 1 minute is too short or too long? If <span class="math notranslate nohighlight">\(h\)</span> was one second instead of one minute, one hour would be split into 3600 pieces. The simulation time would go up, but would the <em>accuracy</em> of our calculation be any better? The goal of any numerical simulation is to keep the numerical error to an acceptable level. We will never get rid of it as you will see in this chapter.</p>
<p>Most physical systems are described in terms of <em>differential equations</em>. A differential equation describe how a physical phenomenon evolves in space and time. The solution to a differential equation is a function of space and/or time. The function could describe the temperature evolution of the earth, it could be growth of cancer cells, the water pressure in an oil reservoir, the list is endless. If we can solve the model analytically, the answer is given in terms of a known function. Most of the models cannot be solved analytically, then we have to rely on computers to help us. The computer does not have any concept of continuous functions, a function is always evaluated at some specific points in space and/or time.</p>
<div class="admonition-numerical-errors admonition">
<p class="admonition-title">Numerical errors</p>
<p id="index-0">To represent a function of space and/or time in a computer, the function needs to be discretized. When a function is discretized it leads to discretization errors. The difference between the &quot;true&quot; answer and the answer obtained from a practical (numerical) calculation is called the <em>numerical error</em>.</p>
</div>
<p>When we divide space and time into finite pieces to represent them in a computer, a natural question to ask is how many pieces do we need. Consider an almost trivial example, let say you want visualize the function <span class="math notranslate nohighlight">\(f(x)=\sin x\)</span>. To do this we need to choose where, which values of <span class="math notranslate nohighlight">\(x\)</span>, we want to evaluate our function. Clearly, we want to use as few points as possible but still capture shape of the true function.
In figure <a class="reference internal" href="#fig-taylor-sinx"><span class="std std-ref">A plot of  \( sin x \)  for different spacing of the  \( x \) -values</span></a>, we have plotted <span class="math notranslate nohighlight">\(\sin x\)</span> for various discretization (spacing between the points) in the interval <span class="math notranslate nohighlight">\([-\pi,\pi]\)</span>.</p>
<div class="figure align-default" id="id2">
<span id="fig-taylor-sinx"></span><a class="reference internal image-reference" href="_images/func_plot.png"><img alt="_images/func_plot.png" src="_images/func_plot.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text"><em>A plot of  \( sin x \)  for different spacing of the  \( x \) -values</em></span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>From the figure we see that in some areas only a couple of points are needed in order to
represent the function well, and in some areas more points are needed. To state it more clearly; between <span class="math notranslate nohighlight">\([-1,1]\)</span> a linear function (few points) approximate <span class="math notranslate nohighlight">\(\sin x\)</span> well,
whereas in the area where the derivative of the function changes more rapidly e.g. in <span class="math notranslate nohighlight">\([-2,-1]\)</span>, we need the points to be more closely spaced to capture the behavior of the true function.</p>
<p>What is a <em>good representation</em> representation of the true function? We cannot rely on visual inspection every time, and most of the time we do not know the true answer so we would not know what to compare it with. In the next section we will show how Taylor polynomial representation of a function is a natural starting point to answer this question.</p>
</div>
<div class="section" id="taylor-polynomial-approximation">
<h2>Taylor Polynomial Approximation<a class="headerlink" href="#taylor-polynomial-approximation" title="Permalink to this headline">¶</a></h2>
<p>How can we evaluate numerical errors if we do not know the true answer? There are at least two answers to this</p>
<ol class="arabic simple">
<li><p>The pragmatic engineering approach is to do a simulation with a coarse grid, then refine the grid until the solution does not change very much. This is perfectly fine <em>if you know that your numerical code is bug free</em>, because even if the simulation converges to a solution we do not know if it is the <em>true solution</em>. In too many cases this is not so. Therefore even in well tested industrial codes, it is always good to test them on a simple test case where you know the exact solution.</p></li>
<li><p>Taylors formula can be used to represent any continuous function with continuous derivatives or most solutions to a mathematical model. Taylors formula gives us an estimate of the numerical error introduced when we divide space and time into finite pieces.</p></li>
</ol>
<p>There are many ways of representing a function, <span class="math notranslate nohighlight">\(f(x)\)</span>, like Fourier series, Legendre polynomials, but perhaps one of the most widely used is Taylor polynomials.
Taylor series are perfect for computers, simply because it makes it possible to evaluate any function with a set of limited operations: <em>addition, subtraction, and multiplication</em>. Let us start off with the formal definition:</p>
<div class="admonition-taylor-polynomial admonition">
<p class="admonition-title">Taylor polynomial</p>
<p id="index-1">The Taylor polynomial, <span class="math notranslate nohighlight">\(P_n(x)\)</span> of degree <span class="math notranslate nohighlight">\(n\)</span> of a function <span class="math notranslate nohighlight">\(f(x)\)</span> at the point <span class="math notranslate nohighlight">\(c\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[P_n(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots+\frac{f^{(n)}(c)}{n!}(x-c)^n\nonumber\]</div>
</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-taylori">
\[\tag{1}
=\sum_{k=0}^n\frac{f^{(k)}(c)}{k!}(x-c)^k.\]</div>
<p>Note that <span class="math notranslate nohighlight">\(x\)</span> can be anything, space, time, temperature etc. If the series is around the point <span class="math notranslate nohighlight">\(c=0\)</span>, the Taylor polynomial <span class="math notranslate nohighlight">\(P_n(x)\)</span> is often called a Maclaurin polynomial. If the series converge (i.e. that the higher order terms approach zero), then we can represent the function <span class="math notranslate nohighlight">\(f(x)\)</span> with its corresponding Taylor series around the point <span class="math notranslate nohighlight">\(x=c\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-taylor">
\[\tag{2}
f(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots
    =\sum_{k=0}^\infty\frac{f^{(k)}}{k!}(x-c)^k.\]</div>
<div class="admonition-the-magic-of-taylors-formula admonition">
<p class="admonition-title">The magic of Taylors formula</p>
<p>Taylors formula, equation <a class="reference internal" href="#eq-eq-taylor-taylor"><span class="std std-ref">(2)</span></a>, states that if we know the function value and its derivative <em>in a single point $c$</em>, we can estimate the function everywhere <em>using only  information from the single point $c$</em>. How can this be, how can information in a single point be used to predict the behavior of the function everywhere? One way of thinking about it could be to imagine an object moving in a constant gravitational field without air resistance. Newtons laws then tells us that  if we know the starting point e.g. (<span class="math notranslate nohighlight">\(x(0)\)</span>), the velocity (<span class="math notranslate nohighlight">\(v=dx/dt\)</span>), and the acceleration (<span class="math notranslate nohighlight">\(a=dv/dt=d^2x/dt^2\)</span>) in that point we can predict the trajectory of the object. This trajectory is exactly the first terms in Taylors formula, <span class="math notranslate nohighlight">\(x(t)=x(0) + vt+at^2/2\)</span>.</p>
</div>
<p>An example of how Taylors formula works for a known function, can be seen in figure <a class="reference internal" href="#fig-mac-sin"><span class="std std-ref">Nine first terms of the Maclaurin series of \sin x</span></a>, where we show the first nine terms in the Maclaurin series for <span class="math notranslate nohighlight">\(\sin x\)</span> (all even terms are zero).</p>
<div class="figure align-default" id="id3">
<span id="fig-mac-sin"></span><a class="reference internal image-reference" href="_images/mac_sin.png"><img alt="_images/mac_sin.png" src="_images/mac_sin.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text">Nine first terms of the Maclaurin series of <span class="math notranslate nohighlight">\(\sin x\)</span></span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>Notice that close to <span class="math notranslate nohighlight">\(x=0\)</span> we only need one term, as we move further away from this point more and more term needs to be added. Thus, Taylors formula is only exact if we include an infinite number of terms. In practice we only include a limited number of terms and truncate the series up to a given order. Luckily, Taylors formula include an estimate of the error we do when we truncate the series.</p>
<div class="admonition-truncation-error-in-taylors-formula admonition" id="index-2">
<p class="admonition-title">Truncation error in Taylors formula</p>
<div class="math notranslate nohighlight" id="index-3">
\[R_n(x)=f(x)-P_n(x)=\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-c)^{n+1}\nonumber\]</div>
</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-error">
\[ \begin{align}\begin{aligned} \tag{3}
 =\frac{1}{n!}\int_c^x(x-\tau)^{n}f^{(n+1)}(\tau)d\tau,\\\
Notice that the mathematical formula is basically the next order term ( \( n+1 \) ) in the Taylor series, but with  \( f^{(n+1)}(c)\to f^{(n+1)}(\eta) \) .  \( \eta \)  is an (unknown) value in the domain  \( [x,c] \) .\end{aligned}\end{align} \]</div>
<p>Notice that if <span class="math notranslate nohighlight">\(c\)</span> is very far from <span class="math notranslate nohighlight">\(x\)</span> the truncation error increases. The fact that we do not know the value of <span class="math notranslate nohighlight">\(\eta\)</span> is usually not a problem, in many cases we just replace <span class="math notranslate nohighlight">\(f(\eta)\)</span> with the maximum value it can take on the domain. Equation <a class="reference internal" href="#eq-eq-taylor-error"><span class="std std-ref">(3)</span></a> gives us an direct estimate of discretization error.</p>
<div class="admonition-example-evaluate-math-sin-x admonition">
<p class="admonition-title">Example: evaluate <span class="math notranslate nohighlight">\(\sin x\)</span></p>
<p>Whenever you do e.g. <code class="docutils literal notranslate"><span class="pre">np.sin(1)</span></code> in Python or an equivalent statement in another language, Python has to tell the computer how to evaluate <span class="math notranslate nohighlight">\(\sin x\)</span> at <span class="math notranslate nohighlight">\(x=1\)</span>. Write a Python code that calculates <span class="math notranslate nohighlight">\(\sin x\)</span> up to a user specified accuracy.</p>
<p><strong>Solution</strong>
The Maclaurin series of <span class="math notranslate nohighlight">\(\sin x\)</span> is:</p>
</div>
<div class="math notranslate nohighlight" id="eq-sin">
<span id="index-4"></span>\[ \begin{align}\begin{aligned} \tag{4}
 \sin x = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots=\sum_{k=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}.\\If we want to calculate  \( \sin x \)  to a precision lower than a specified value we can do it as follows:\\.. code-block:: python\\    import numpy as np\\    # Sinus implementation using the Maclaurin Serie
    # By setting a value for eps this value will be used
    # if not provided
    def my_sin(x,eps=1e-16):
        f = power = x
        x2 = x*x
        sign = 1
        i=0
        while(power&gt;=eps):
            sign = - sign
            power *= x2/(2*i+2)/(2*i+3)
            f += sign*power
            i += 1
        print('No function evaluations: ', i)
        return f\\    x=0.8
    eps = 1e-9
    print(my_sin(x,eps), 'error = ', np.sin(x)-my_sin(x,eps))\\This implementation needs some explanation:\\* The error term is given in equation :ref:`(3) &lt;Eq:eq:taylor:error&gt;`, and it is an even power in  \( x \) . We do not which  \( \eta \)  to use in equation :ref:`(3) &lt;Eq:eq:taylor:error&gt;`, instead we simply say that the error in our estimate is smaller than the highest order term. Thus, we stop the evaluation if the highest order term in the series is lower than the uncertainty. Note that the final error has to be smaller as the higher order terms in any convergent series is smaller than the previous.  Our estimate should then always be better than the specified accuracy.\\* We evaluate the polynomials in the Taylor series by using the previous values too avoid too many multiplications within the loop, we do this by using the following identity:\\.. math::\\          \sin x=\sum_{k=0}^{\infty} (-1)^nt_n, \text{ where: } t_n\equiv\frac{x^{2n+1}}{(2n+1)!}, \text{ hence :}\nonumber\\
.. math::\\          t_{n+1}=\frac{x^{2(n+1)+1}}{(2(n+1)+1)!}=\frac{x^{2n+1}x^2}{(2n+1)! (2n+2)(2n+3)}\nonumber\\
.. _Eq:_auto1:\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\tag{5}
=t_n\frac{x^2}{(2n+2)(2n+3)}\]</div>
<div class="section" id="evaluation-of-polynomials">
<h3>Evaluation of polynomials<a class="headerlink" href="#evaluation-of-polynomials" title="Permalink to this headline">¶</a></h3>
<p>How to evaluate a polynomial of the type: <span class="math notranslate nohighlight">\(p_n(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n\)</span>? We already saw a hint in the previous section that it can be done in different ways. One way is simply to
do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">pol</span> <span class="o">=</span> <span class="n">pol</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="n">i</span>
</pre></div>
</div>
<p>Note that there are <span class="math notranslate nohighlight">\(n\)</span> additions, whereas there are <span class="math notranslate nohighlight">\(1 + 2 +3+\cdots+n=n(n+1)/2\)</span> multiplications for all the iterations. Instead of evaluating the powers all over in
each loop, we can use the previous calculation to save the number of multiplications:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span>
<span class="n">power</span> <span class="o">=</span> <span class="n">x</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">power</span>  <span class="o">=</span> <span class="n">power</span><span class="o">*</span><span class="n">x</span>
    <span class="n">pol</span>    <span class="o">=</span> <span class="n">pol</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">power</span>
</pre></div>
</div>
<p>In this case there are still <span class="math notranslate nohighlight">\(n\)</span> additions, but now there are <span class="math notranslate nohighlight">\(2n-1\)</span> multiplications. For <span class="math notranslate nohighlight">\(n=15\)</span>, this amounts to 120 for the first, and 29 for the second method.
Polynomials can also be evaluated using <em>nested multiplication</em>:</p>
<div class="math notranslate nohighlight">
\[p_1  = a_0+a_1x\nonumber\]</div>
<div class="math notranslate nohighlight">
\[p_2  = a_0+a_1x+a_2x^2=a_0+x(a_1+a_2x)\nonumber\]</div>
<div class="math notranslate nohighlight">
\[p_3  = a_0+a_1x+a_2x^2+a_3x^3=a_0+x(a_1+x(a_2+a_3x))\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-auto2">
\[\tag{6}
\vdots\]</div>
<p>and so on. This can be implemented as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">pol</span>  <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">pol</span><span class="o">*</span><span class="n">x</span>
</pre></div>
</div>
<p>In this case we only have <span class="math notranslate nohighlight">\(n\)</span> multiplications. So if you know beforehand exactly how many terms is needed to calculate the series, this method would be the preferred method, and is implemented in NumPy as <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html#r138ee7027ddf-1">polyval</a>.</p>
</div>
</div>
<div class="section" id="calculating-numerical-derivatives-of-functions">
<h2>Calculating Numerical Derivatives of Functions<a class="headerlink" href="#calculating-numerical-derivatives-of-functions" title="Permalink to this headline">¶</a></h2>
<p>As stated earlier many models are described by differential equations. Differential equations contains derivatives, and we need to tell the computer how to calculate those. By using a simple transformation, <span class="math notranslate nohighlight">\(x\to x+h\)</span> and <span class="math notranslate nohighlight">\(c\to x\)</span> (hence <span class="math notranslate nohighlight">\(x-c\to h\)</span>), Taylors formula in equation <a class="reference internal" href="#eq-eq-taylor-taylor"><span class="std std-ref">(2)</span></a> can be written</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-t">
\[\tag{7}
f(x+h)=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2+\cdots.\]</div>
<p>This is useful because this equation contains the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> on the right hand side. To be even more explicit let us truncate the series to a certain power. Remember that you can always do this but we need to replace <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(\eta\)</span> in the last term we choose to keep</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-t3">
\[\tag{8}
f(x+h)=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(\eta)h^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\in[x,x+h]\)</span>. Solving this equation with respect to <span class="math notranslate nohighlight">\(f^\prime(x)\)</span> gives us</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-fd">
\[\tag{9}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}-\frac{1}{2}f^{\prime\prime}(\eta)h.\]</div>
<p>Note that if <span class="math notranslate nohighlight">\(h\to0\)</span>, this expression is equal to the definition of the derivative. The beauty of equation <a class="reference internal" href="#eq-eq-taylor-fd"><span class="std std-ref">(9)</span></a> is that it contains an expression for the error we make <em>when  \( h \)  is not zero</em>. Equation <a class="reference internal" href="#eq-eq-taylor-fd"><span class="std std-ref">(9)</span></a> is usually called the <em>forward difference</em>
.. index:: forward difference</p>
<p>. As you might guess, we can also choose to use the <em>backward difference</em>
.. index:: backward difference</p>
<blockquote>
<div><p>by simply replacing <span class="math notranslate nohighlight">\(h\to-h\)</span>. Is equation <a class="reference internal" href="#eq-eq-taylor-fd"><span class="std std-ref">(9)</span></a> the only formula for the derivative? The answer is no, and we are going to derive the formula for the <em>central difference</em></p>
</div></blockquote>
<p id="index-5">, by writing Taylors formula for <span class="math notranslate nohighlight">\(x+h\)</span> and <span class="math notranslate nohighlight">\(x-h\)</span> up to the third order</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-c1">
\[\tag{10}
f(x+h)=f(x)+f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2+\frac{1}{3!}f^{(3)}(\eta_1)h^3,\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-c2">
\[\tag{11}
f(x-h)=f(x)-f^\prime(x)h+\frac{1}{2}f^{\prime\prime}(x)h^2-\frac{1}{3!}f^{(3)}(\eta_2)h^3.\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta_1\in[x,x+h]\)</span>, and <span class="math notranslate nohighlight">\(\eta_2\in[x-h,x]\)</span>. Subtracting  equation <a class="reference internal" href="#eq-eq-taylor-c1"><span class="std std-ref">(10)</span></a> and <a class="reference internal" href="#eq-eq-taylor-c2"><span class="std std-ref">(11)</span></a>, we get the following expression for the central difference
.. index:: central difference</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-cd">
\[\tag{12}
f^\prime(x)=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\in[x-h,x+h]\)</span>. Note that the error term in this equation is <em>one order higher</em> than in equation <a class="reference internal" href="#eq-eq-taylor-fd"><span class="std std-ref">(9)</span></a>, meaning that it is expected to be more accurate. In figure <a class="reference internal" href="#fig-taylor-fd"><span class="std std-ref">A graphical interpretation of the forward and central difference formula</span></a> there is a graphical interpretation of the finite difference approximations to the derivative.</p>
<div class="figure align-default" id="id4">
<span id="fig-taylor-fd"></span><a class="reference internal image-reference" href="_images/fd.png"><img alt="_images/fd.png" src="_images/fd.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><em>A graphical interpretation of the forward and central difference formula</em></span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>We are also now in the position to derive a formula for the second order derivative. Instead of subtracting equation <a class="reference internal" href="#eq-eq-taylor-c1"><span class="std std-ref">(10)</span></a> and <a class="reference internal" href="#eq-eq-taylor-c2"><span class="std std-ref">(11)</span></a>, we can add them. Then the first order derivative disappear and we are left with an expression for the second derivative</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-2der">
\[\tag{13}
f^{\prime\prime}(x) = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}- \frac{h^2}{12}f^{(4)}(\eta)
    ,\]</div>
<div class="admonition-example-calculate-the-numerical-derivative-and-second-derivative-of-math-sin-x admonition">
<p class="admonition-title">Example: calculate the numerical derivative and second derivative of <span class="math notranslate nohighlight">\(\sin x\)</span></p>
<p>Choose a specific point, e.g. <span class="math notranslate nohighlight">\(x=1\)</span>, and calculate the numerical error for various values of the step size <span class="math notranslate nohighlight">\(h\)</span>.
<strong>Solution:</strong>
The derivative of <span class="math notranslate nohighlight">\(\sin x\)</span> is <span class="math notranslate nohighlight">\(\cos x\)</span>, we can calculate the numerical derivatives using Python</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">):</span>
 <span class="sd">&quot;&quot;&quot;</span>
<span class="sd"> calculates the forward difference approximation to</span>
<span class="sd"> the numerical derivative of f in x</span>
<span class="sd"> &quot;&quot;&quot;</span>
 <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">h</span>

<span class="k">def</span> <span class="nf">fc</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">):</span>
 <span class="sd">&quot;&quot;&quot;</span>
<span class="sd"> calculates the central difference approximation to</span>
<span class="sd"> the numerical derivative of f in x</span>
<span class="sd"> &quot;&quot;&quot;</span>
 <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">))</span><span class="o">/</span><span class="n">h</span>

<span class="k">def</span> <span class="nf">fdd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">):</span>
 <span class="sd">&quot;&quot;&quot;</span>
<span class="sd"> calculates the numerical second order derivative</span>
<span class="sd"> of f in x</span>
<span class="sd"> &quot;&quot;&quot;</span>
 <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">h</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="mi">1</span>
<span class="n">h</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">fd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">)),</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;forward difference&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">fc</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">)),</span><span class="s1">&#39;-x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;central difference&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">fdd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">)),</span><span class="s1">&#39;-*&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;second derivative&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Numerical error of derivatives of  \( sin x \)  for various step sizes</span></a> you can see the figure produced by the code above.</p>
</div>
<div class="figure align-default" id="id5">
<span id="fig-taylor-df2"></span><a class="reference internal image-reference" href="_images/df2_mod.png"><img alt="_images/df2_mod.png" src="_images/df2_mod.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><em>Numerical error of derivatives of  \( sin x \)  for various step sizes</em></span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>There are several important lessons from figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Numerical error of derivatives of  \( sin x \)  for various step sizes</span></a>
1. When the step size is high and decreasing (from right to left in the figure), we clearly see that the numerical error <em>decreases</em>.</p>
<ol class="arabic simple" start="2">
<li><p>The numerical error scales as expected from right to left. The forward difference formula scales as <span class="math notranslate nohighlight">\(h\)</span>, i.e. decreasing the step size by 10 reduces the numerical error by 10. The central difference and second order derivative formula scales as <span class="math notranslate nohighlight">\(h^2\)</span>, reducing the step size by 10 reduces the numerical error by 100</p></li>
<li><p>At a certain point the numerical error start to <em>increase</em>. For the forward difference formula this happens at  \(  10^{-8} \) .</p></li>
</ol>
<p>The numerical error has a minimum, <em>it does not continue to decrease when  \( h \)  decreases</em>. The explanation for this behavior is two competing effects: <em>truncation errors</em> and <em>roundoff errors</em>. The truncation errors have already been discussed in great detail, in the next section we will explain roundoff errors.</p>
<div class="section" id="roundoff-errors">
<h3>Roundoff Errors<a class="headerlink" href="#roundoff-errors" title="Permalink to this headline">¶</a></h3>
<p id="index-6">In a computer a floating point number,$x$, is represented as:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-sci2">
\[\tag{14}
x=\pm q2^m.\]</div>
<p>This is very similar to our usual scientific notation where we represents large (or small numbers) as <span class="math notranslate nohighlight">\(\pm q E m=\pm q 10^{m}\)</span>. The processor in a computer handles a chunk of bits at one time, this chunk of bit is usually termed <em>word</em>. The number of bits (or byte which almost always means a group of eight bits) in a word is handled as a unit by a processor.
Most modern computers uses 64-bits (8 bytes) processors. We are not going too much into all the details, the most important message is that the units handled by the processor are <em>finite</em>. Thus we cannot, in general, store numbers in a computer with infinite accuracy.</p>
<div class="admonition-machine-precision admonition">
<p class="admonition-title">Machine Precision</p>
<p id="index-7">Machine precision, <span class="math notranslate nohighlight">\(\epsilon_M\)</span> is the smallest number we can add to one and get something different than one, i.e. <span class="math notranslate nohighlight">\(1+\epsilon_M&gt;1\)</span>. For a 64-bits computer this value is <span class="math notranslate nohighlight">\(\epsilon_M=2^{-52}\simeq2.2210^{-16}\)</span>.</p>
</div>
<p>In the next section we explain exactly why the machine precision has this value, but if you just accept this for a moment we can demonstrate why the machine precision is important and why you need to care about it. First just to convince you that the machine precision has the value of <span class="math notranslate nohighlight">\(2^{-52}\)</span> in your computer you can do the following in Python</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="o">**-</span><span class="mi">52</span><span class="p">)</span> <span class="c1"># prints a value larger than 1</span>
<span class="nb">print</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="o">**-</span><span class="mi">53</span><span class="p">)</span> <span class="c1"># prints 1.0</span>
</pre></div>
</div>
<p>Next, consider the simple calculation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">=</span><span class="mf">0.1</span><span class="o">+</span><span class="mf">0.2</span>
<span class="n">b</span><span class="o">=</span><span class="mf">0.3</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">==</span><span class="n">b</span><span class="p">)</span> <span class="c1"># gives False</span>
</pre></div>
</div>
<p>Why is <code class="docutils literal notranslate"><span class="pre">a==b</span></code> false, the calculation involves only numbers with one decimal? The reason is that the computer uses the binary system, and in the binary system there is no way of representing 0.2 and 0.3 with a finite number of bits, as an example 0.2 in the binary system is</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-02">
\[\tag{15}
0.2_{10}=0.0011001100\ldots_2 (=2^{-3}+2^{-4}+2^{-7}+2^{-8}+2^{-11}+\cdots)\]</div>
<p>Note that we use the subscript <span class="math notranslate nohighlight">\(_{10}\)</span> and <span class="math notranslate nohighlight">\(_2\)</span> to represent the decimal and binary system respectively.
Thus in the computer 0.2 will be represented as <span class="math notranslate nohighlight">\(0.1999\ldots\)</span> and when we add 0.1 we will get a number really close to 0.3 but not equal to 0.3. Some floats have an exact binary representation e.g. <span class="math notranslate nohighlight">\(0.125_{10}=2^{-8}_{10}=0.00000001_2\)</span>. Thus the following code will produce the expected result</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">=</span><span class="mf">0.125</span><span class="o">+</span><span class="mf">0.25</span>
<span class="n">b</span><span class="o">=</span><span class="mf">0.375</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">==</span><span class="n">b</span><span class="p">)</span> <span class="c1"># gives True</span>
</pre></div>
</div>
<div class="admonition-comparing-two-floats admonition">
<p class="admonition-title">Comparing two floats</p>
<p>Whenever you want to compare if two floats, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, are equal in a computer program, you should never do <span class="math notranslate nohighlight">\(a==b\)</span> because of roundoff errors. Rather you should choose a variant of <span class="math notranslate nohighlight">\(|a-b|&lt;\epsilon\)</span>, where you check if the numbers are <em>close enough</em>. In practice you also might want to normalize the values and do <span class="math notranslate nohighlight">\(|1-b/a|&lt;\epsilon\)</span>.</p>
</div>
<p>The roundoff errors can also play a very big role in calculations, it is particularly apparent when subtracting two numbers of similar magnitude as illustrated in the following code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">53</span>
<span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">h</span>
<span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span>
<span class="nb">print</span><span class="p">((</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">h</span><span class="p">)</span> <span class="c1"># analytical result is 2</span>
</pre></div>
</div>
<p>The calculation above is very similar to the calculation done when evaluating derivatives, and if you run the code you will see that Python does not give the expected value of 2.</p>
<div class="admonition-choosing-the-right-step-size admonition">
<p class="admonition-title">Choosing the right step size</p>
<p>A step size that is too low will give higher numerical error because roundoff errors dominate the numerical error.</p>
</div>
<p>At the end we will mention a simple trick that you can use sometimes to avoid roundoff errors <span id="id1">[Ref2]</span>. In practice we can never get rid of roundoff errors in the calculation <span class="math notranslate nohighlight">\(f(x+h)\)</span>, but since we can choose the step size <span class="math notranslate nohighlight">\(h\)</span> we can choose to choose values such that <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+h\)</span> differ by an exact binary number</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="mi">1</span>
<span class="n">h</span><span class="o">=</span><span class="mf">0.0002</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">h</span>
<span class="n">h</span><span class="o">=</span><span class="n">temp</span><span class="o">-</span><span class="n">x</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># improved value of h with exact binary representation</span>
</pre></div>
</div>
<p>In the next sections we will show why <span class="math notranslate nohighlight">\(\epsilon_M=2^{-52}\)</span>, and why a finite word size leads necessary has to imply a maximum and minimum number.
Binary numbers
~~~~~~~~~~~~~~</p>
<p>Binary numbers are used in computers because processors are made of billions of transistors, the end states of a transistor is off or on, representing a 0 or 1 in the binary system. Assume, for simplicity, that we have a processor that uses a word size of 4 bits (instead of 64 bits). How many <em>unsigned</em> (positive) integers can we represent in this processor? Lets write down all the possible combinations, of ones and zeros and also do the translation from base 2 numerical system to base 10 numerical system:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-bin4">
\[\begin{split}\tag{16}
\begin{matrix}
    0&amp;0&amp;0&amp;0=0\cdot 2^3+0\cdot 2^2+0\cdot 2^1+0\cdot 2^0=0\\
    0&amp;0&amp;0&amp;1=0\cdot 2^3+0\cdot 2^2+0\cdot 2^1+1\cdot 2^0=1\\
    0&amp;0&amp;1&amp;0=0\cdot 2^3+0\cdot 2^2+1\cdot 2^1+0\cdot 2^0=2\\
    0&amp;0&amp;1&amp;1=0\cdot 2^3+0\cdot 2^2+1\cdot 2^1+1\cdot 2^0=3\\
    0&amp;1&amp;0&amp;0=0\cdot 2^3+1\cdot 2^2+0\cdot 2^1+0\cdot 2^0=4\\
    0&amp;1&amp;0&amp;1=0\cdot 2^3+1\cdot 2^2+0\cdot 2^1+1\cdot 2^0=5\\
    0&amp;1&amp;1&amp;0=0\cdot 2^3+1\cdot 2^2+1\cdot 2^1+0\cdot 2^0=6\\
    0&amp;1&amp;1&amp;1=0\cdot 2^3+1\cdot 2^2+1\cdot 2^1+1\cdot 2^0=7\\
    1&amp;0&amp;0&amp;0=1\cdot 2^3+0\cdot 2^2+0\cdot 2^1+0\cdot 2^0=8\\
    1&amp;0&amp;0&amp;1=1\cdot 2^3+0\cdot 2^2+0\cdot 2^1+1\cdot 2^0=9\\
    1&amp;0&amp;1&amp;0=1\cdot 2^3+0\cdot 2^2+1\cdot 2^1+0\cdot 2^0=10\\
    1&amp;0&amp;1&amp;1=1\cdot 2^3+0\cdot 2^2+1\cdot 2^1+1\cdot 2^0=11\\
    1&amp;1&amp;0&amp;0=1\cdot 2^3+1\cdot 2^2+0\cdot 2^1+0\cdot 2^0=12\\
    1&amp;1&amp;0&amp;1=1\cdot 2^3+1\cdot 2^2+0\cdot 2^1+1\cdot 2^0=13\\
    1&amp;1&amp;1&amp;0=1\cdot 2^3+1\cdot 2^2+1\cdot 2^1+0\cdot 2^0=14\\
    1&amp;1&amp;1&amp;1=1\cdot 2^3+1\cdot 2^2+1\cdot 2^1+1\cdot 2^0=15
    \end{matrix}
    .\end{split}\]</div>
<p>Hence, with a 4 bits word size, we can represent <span class="math notranslate nohighlight">\(2^4=16\)</span> integers. The largest number is <span class="math notranslate nohighlight">\(2^4-1=15\)</span>, and the smallest is zero. What about negative numbers? If we still keep to a 4 bits word size, there are still <span class="math notranslate nohighlight">\(2^4=16\)</span> numbers, but we distribute them differently. The common way to do it is to reserve the first bit to be a <em>sign</em> bit, a &quot;0&quot; is positive and &quot;1&quot; is negative, i.e. <span class="math notranslate nohighlight">\((-1)^0 = 1\)</span>, and <span class="math notranslate nohighlight">\((-1)^1=-1\)</span>. Replacing the first bit with a sign bit in equation <a class="reference internal" href="#eq-eq-taylor-bin4"><span class="std std-ref">(16)</span></a>, we get the following sequence of numbers 0,1,2,3,4,5,6,7,-0,-1,-2,-3,-4,-5,-6,-7. The &quot;-0&quot;, might seem strange but is used in the computer to extend the real number line <span class="math notranslate nohighlight">\(1/0=\infty\)</span>, whereas <span class="math notranslate nohighlight">\(1/-0=-\infty\)</span>. In general when there are <span class="math notranslate nohighlight">\(m\)</span> bits, we have a total of <span class="math notranslate nohighlight">\(2^m\)</span> numbers. If we include negative numbers, we can choose to have <span class="math notranslate nohighlight">\(2^{m-1}-1\)</span>, negative, and <span class="math notranslate nohighlight">\(2^{m-1}-1\)</span> positive numbers, negative zero and positive zero, i.e. <span class="math notranslate nohighlight">\(2^{m-1}-1+2^{m-1}-1+1+1=2^m\)</span>.</p>
<p>What about real numbers? As stated earlier we use the scientific notation as in equation <a class="reference internal" href="#eq-eq-taylor-sci2"><span class="std std-ref">(14)</span></a>, but still the scientific notation might have a real number in front, e.g. <span class="math notranslate nohighlight">\(1.25\cdot 10^{-3}\)</span>. To represent the number <span class="math notranslate nohighlight">\(1.25\)</span> in binary format we use a decimal separator, just as with base 10. In this case 1.25 is 1.01 in binary format</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-b2fr">
\[\tag{17}
1.01=1\cdot 2^0 + 0\cdot 2^{-1}+1\cdot 2^{-2}=1 + 0 + 0.25=1.25.\]</div>
<p>The scientific notation is commonly referred to as <em>floating point representation</em>. The term &quot;floating point&quot; is used because the decimal point is not in the same place, in contrast to fixed point where the decimal point is always in the same place. To store the number 1e-8=0.00000001 in floating point format, we only need to store 1 and -8 (and possibly the sign), whereas in fixed point format we need to store all 9 numbers.  In equation <a class="reference internal" href="#eq-eq-taylor-bin4"><span class="std std-ref">(16)</span></a> we need to spend one bit to store the sign, leaving (in the case of 4 bits word size) three bits to be distributed among the <em>mantissa</em>, <span class="math notranslate nohighlight">\(q\)</span>, and the exponent, <span class="math notranslate nohighlight">\(m\)</span>. It is not given how many bits should be used for the mantissa and the exponent. Thus there are choices to be made, and all modern processors uses the same standard, the <a class="reference external" href="https://standards.ieee.org/standard/754-1985.html">IEEE Standard 754-1985</a>.</p>
<div class="section" id="floating-point-numbers-and-the-ieee-754-1985-standard">
<h4>Floating point numbers and the IEEE 754-1985 standard<a class="headerlink" href="#floating-point-numbers-and-the-ieee-754-1985-standard" title="Permalink to this headline">¶</a></h4>
<p id="index-8">A 64 bits word size is commonly referred to as <em>double precision</em>, whereas a 32 bits word size is termed <em>single precision</em>. In the following we will consider a 64 bits word size. We would like to know: what is the roundoff error, what is the largest number that can be represented in the computer, and what is the smallest number? Almost all floating point numbers are represented in <em>normalized</em> form. In normalized form the mantissa is written as <span class="math notranslate nohighlight">\(M=1.F\)</span>, and it is only <span class="math notranslate nohighlight">\(F\)</span> that is stored,   <span class="math notranslate nohighlight">\(F\)</span> is termed the <em>fraction</em>. We will return to the special case of some of the unnormalized numbers later. In the IEEE standard one bit is reserved for the sign, 52 for the fraction (<span class="math notranslate nohighlight">\(F\)</span>) and 11 for the exponent (<span class="math notranslate nohighlight">\(m\)</span>), see figure <a class="reference internal" href="#fig-taylor-64bit"><span class="std std-ref">Representation of a 64 bits floating point number according to the IEEE 754-1985 standard. For a 32 bits floating point number, 8, is reserved for the exponent and 23 for the fraction</span></a> for an illustration.</p>
<div class="figure align-default" id="id6">
<span id="fig-taylor-64bit"></span><a class="reference internal image-reference" href="_images/64bit.png"><img alt="_images/64bit.png" src="_images/64bit.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><em>Representation of a 64 bits floating point number according to the IEEE 754-1985 standard. For a 32 bits floating point number, 8, is reserved for the exponent and 23 for the fraction</em></span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>The exponent must be positive to represent numbers with absolute value larger than one, and negative to represent numbers with absolute value less than one.  To make this more explicit the simple formula in equation <a class="reference internal" href="#eq-eq-taylor-sci2"><span class="std std-ref">(14)</span></a> is rewritten:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-ieee">
\[\tag{18}
\pm q 2^{E-e}.\]</div>
<p>The number <span class="math notranslate nohighlight">\(e\)</span> is called the <em>bias</em> and has a fixed value, for 64 bits it is <span class="math notranslate nohighlight">\(2^{11-1}-1=1023\)</span> (32-bits: <span class="math notranslate nohighlight">\(e=2^{8-1}-1=127\)</span>). The number <span class="math notranslate nohighlight">\(E\)</span> is represented by 11 bits and can thus take on values from 0 to <span class="math notranslate nohighlight">\(2^11-1=2047\)</span>. If we have an exponent of e.g. -3, the computer adds 1023 to that number and store the number 1020. Two numbers are special numbers and reserved to represent infinity and zero, <span class="math notranslate nohighlight">\(E=0\)</span> and <span class="math notranslate nohighlight">\(E=2047\)</span>. Thus <em>the largest and smallest possible numerical value of the exponent is: 2046-1023=1023, and 1-1023=-1022, respectively</em>. The fraction of a normalized floating point number takes on values from <span class="math notranslate nohighlight">\(1.000\ldots 00\)</span> to <span class="math notranslate nohighlight">\(1.111\ldots 11\)</span>. Thus the lowest normalized number is</p>
<div class="math notranslate nohighlight" id="eq-auto3">
\[\tag{19}
1.000 + \text{ (49 more zeros)}\cdot 2^{-1022}=2^0\cdot2^{-1022}{\nonumber}\]</div>
<div class="math notranslate nohighlight">
\[=2.2250738585072014\cdot 10^{-308}.
label{}\]</div>
<p>It is possible to represent smaller numbers than <span class="math notranslate nohighlight">\(2.22\cdot10^{-308}\)</span>, by allowing <em>unnormalized</em> values. If the exponent is -1022, then the mantissa can take on values from <span class="math notranslate nohighlight">\(1.000\ldots 00\)</span> to <span class="math notranslate nohighlight">\(0.000\ldots 01\)</span>, but then accuracy is lost. So the smallest possible number is <span class="math notranslate nohighlight">\(2^{-52}\cdot{2^-1022}\simeq4.94\cdot10^{-324}\)</span>.
The highest normalized number is</p>
<div class="math notranslate nohighlight" id="eq-auto4">
\[\tag{20}
1.111 + \text{ (49 more ones)}\cdot2^{1023}=(2^0+2^{-1}+2^{-2}+\cdots+2^{-52})\cdot2^{1023}{\nonumber}\]</div>
<div class="math notranslate nohighlight">
\[=(2-2^{-52})\cdot2^{1023}
=1.7976931348623157\cdot 10^{308}.
label{}\]</div>
<p>If you enter <code class="docutils literal notranslate"><span class="pre">print(1.8*10**(308))</span></code> in Python, the answer will be <code class="docutils literal notranslate"><span class="pre">Inf</span></code>. If you enter <code class="docutils literal notranslate"><span class="pre">print(2*10**(308))</span></code>, Python will (normally) give an answer. This is because
the number <span class="math notranslate nohighlight">\(1.8\cdot10^{308}\)</span> is floating point number, whereas <span class="math notranslate nohighlight">\(2\cdot 10^{308}\)</span> is an <em>integer</em>, and Python does something clever when it comes to representing integers.
Python has a third numeric type called long int, which can use the available memory to represent an integer.</p>
<p>What about the machine precision? The machine precision, <span class="math notranslate nohighlight">\(\epsilon_M\)</span>, is the <em>smallest possible number that can be added to one, and get a number larger than one</em>, i.e. <span class="math notranslate nohighlight">\(1+\epsilon_M&gt;1\)</span>.  The smallest possible value of the mantissa is <span class="math notranslate nohighlight">\(0.000\ldots 01=2^{-52}\)</span>, thus the lowest number must be of the form <span class="math notranslate nohighlight">\(2^{-52}\cdot 2^{m}\)</span>. If the exponent , <span class="math notranslate nohighlight">\(m\)</span>, is lower than 0 then when we add this number to 1, we will only get 1. Thus the machine precision is <span class="math notranslate nohighlight">\(\epsilon_M=2^{-52}=2.22\cdot10^{-16}\)</span> (for 32 bits <span class="math notranslate nohighlight">\(2^{-23}=1.19\cdot10^{-7}\)</span>). In practical terms this means that e.g. the value of <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(3.14159265358979323846264338\ldots\)</span>, but in Python it can only be represented by 16 digits: <span class="math notranslate nohighlight">\(3.141592653589793\)</span>.</p>
</div>
<div class="section" id="roundoff-error-and-truncation-error-in-numerical-derivatives">
<h4>Roundoff error and truncation error in numerical derivatives<a class="headerlink" href="#roundoff-error-and-truncation-error-in-numerical-derivatives" title="Permalink to this headline">¶</a></h4>
<div class="admonition-roundoff-errors admonition">
<p class="admonition-title">Roundoff Errors</p>
<p id="index-9">All numerical floating point operations introduces roundoff errors at each step in the calculation due to finite word size, these errors accumulate in long simulations and introduce random errors in the final results. After <span class="math notranslate nohighlight">\(N\)</span> operations the error is at least <span class="math notranslate nohighlight">\(\sqrt{N}\epsilon_M\)</span> (the square root is a random walk estimate, and we assume that the errors are randomly distributed). The roundoff errors can be much, much higher when numbers of equal magnitude are subtracted. You might be so unlucky that after one operation the answer is completely dominated by roundoff errors.</p>
</div>
<p>The roundoff error when we represent a floating point number <span class="math notranslate nohighlight">\(x\)</span> in the
machine will be of the order <span class="math notranslate nohighlight">\(x/10^{16}\)</span> (<em>not</em> <span class="math notranslate nohighlight">\(10^{-16}\)</span>). In general, when we evaluate a function the error will be of the order
<span class="math notranslate nohighlight">\(\epsilon|f(x)|\)</span>, where <span class="math notranslate nohighlight">\(\epsilon\sim10^{-16}\)</span>. Thus equation <a class="reference internal" href="#eq-eq-taylor-fd"><span class="std std-ref">(9)</span></a> is modified in the following way when we take into account the roundoff errors:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr2">
\[\tag{21}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}\pm\frac{2\epsilon|f(x)|}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\]</div>
<p>we do not know the sign of the roundoff error, so the total error <span class="math notranslate nohighlight">\(R_2\)</span> is:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr3">
\[\tag{22}
R_2=\frac{2\epsilon|f(x)|}{h}+\frac{h}{2}|f^{\prime\prime}(\eta)|.\]</div>
<p>We have put absolute values around the function and its derivative to get the maximal error, it might be the case that the roundoff error cancel part of the
truncation error. However, the roundoff error is random in nature and will change from machine to machine, and each time we run the program.
Note that the roundoff error increases when <span class="math notranslate nohighlight">\(h\)</span> decreases, and the approximation error decreases when <span class="math notranslate nohighlight">\(h\)</span> decreases. This is exactly what we saw in figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Numerical error of derivatives of  \( sin x \)  for various step sizes</span></a>. We can find the
best step size, by differentiating <span class="math notranslate nohighlight">\(R_2\)</span> and put it equal to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{dR_2}{dh}=-\frac{2\epsilon|f(x)|}{h^2}+\frac{1}{2}f^{\prime\prime}(\eta)=0\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr4">
\[\tag{23}
h=2\sqrt{\epsilon\left|\frac{f(x)}{f^{\prime\prime}(\eta)}\right|}\simeq 2\cdot10^{-8},\]</div>
<p>In the last equation we have assumed that <span class="math notranslate nohighlight">\(f(x)\)</span> and its derivative is  \(  1 \) . This step size corresponds to an error of order <span class="math notranslate nohighlight">\(R_2\sim10^{-8}\)</span>.
Inspecting figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Numerical error of derivatives of  \( sin x \)  for various step sizes</span></a> we see that the minimum is located at <span class="math notranslate nohighlight">\(h\sim10^{-8}\)</span>.</p>
<p>We can perform a similar error analysis as we did before, and then we find for equation <a class="reference internal" href="#eq-eq-taylor-cd"><span class="std std-ref">(12)</span></a> and <a class="reference internal" href="#eq-eq-taylor-2der"><span class="std std-ref">(13)</span></a> that the total
numerical error is:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr3b">
\[\tag{24}
R_3=\frac{\epsilon|f(x)|}{h}+\frac{h^2}{6}f^{(3)}(\eta),\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr4b">
\[\tag{25}
R_4=\frac{4\epsilon|f(x)|}{h^2}+\frac{h^2}{12}f^{(4)}(\eta),\]</div>
<p>respectively. Differentiating these two equations with respect to <span class="math notranslate nohighlight">\(h\)</span>, and set the equations equal to zero, we find an optimal step size of
<span class="math notranslate nohighlight">\(h\sim10^{-5}\)</span> for equation <a class="reference internal" href="#eq-eq-taylor-derr3b"><span class="std std-ref">(24)</span></a>, which gives an error of <span class="math notranslate nohighlight">\(R_2\sim 10^{-16}/10^{-5}+(10^{-5})^2/6\simeq10^{-10}\)</span>, and <span class="math notranslate nohighlight">\(h\sim10^{-4}\)</span> for equation
<a class="reference internal" href="#eq-eq-taylor-derr4b"><span class="std std-ref">(25)</span></a>, which gives an error of <span class="math notranslate nohighlight">\(R_4\sim 4\cdot10^{-16}/(10^{-4})^2+(10^{-4})^2/12\simeq10^{-8}\)</span>. Note that we get the surprising result for the first order
derivative in equation <a class="reference internal" href="#eq-eq-taylor-cd"><span class="std std-ref">(12)</span></a>, that a higher step size gives a more accurate result.</p>
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">_</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="._book000.html">Modeling and Computational Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="._book001.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="._book002.html">Writing Python code</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Finite differences</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#numerical-errors">Numerical Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#taylor-polynomial-approximation">Taylor Polynomial Approximation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-of-polynomials">Evaluation of polynomials</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#calculating-numerical-derivatives-of-functions">Calculating Numerical Derivatives of Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#roundoff-errors">Roundoff Errors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="._book004.html">Solving linear systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="._book005.html">Solving nonlinear equations</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="._book002.html" title="previous chapter">Writing Python code</a></li>
      <li>Next: <a href="._book004.html" title="next chapter">Solving linear systems</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Modeling and Computational Engineering.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/._book003.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>